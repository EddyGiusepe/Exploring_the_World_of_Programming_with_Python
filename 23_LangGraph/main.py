#!/usr/bin/env python3
"""
Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro

Resumo curto
============
Aqui uso FastAPI para ter uma interface gr√°fica e assim observar 
o resumo do meu documento.
Tamb√©m, testei usando o Postman. 

NOTA --->  Deixei um print para saber como configurar o 
           Upload do arquivo PDF.

Executar o script:
==================
$ uvicorn main:app --host 0.0.0.0 --port 8000
"""
from fastapi import FastAPI, UploadFile, File
import asyncio
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langgraph.graph import END, MessageGraph
from typing import List, Sequence
from langchain_community.document_loaders import UnstructuredFileLoader

# Transformar o PDF em texto com UnstructuredFileLoader:
def extract_text_with_langchain_pdf(pdf_file):
    loader = UnstructuredFileLoader(pdf_file)
    documents = loader.load()
    pdf_pages_content = '\n'.join(doc.page_content for doc in documents)
    return pdf_pages_content


app = FastAPI(title='ü§ó Usando FastAPI para SUMARIZAR documentos ü§ó',
              version='1.0',
              description="""Data Scientist.: PhD. Eddy Giusepe Chirinos Isidro\n
              Projeto end-to-end para SUMARIZA√á√ÉO""")

# Instanciar o modelo LLM:
llm = ChatOpenAI(model="gpt-4o", temperature=0, streaming=True) # "gpt-4o"   ou    "gpt-3.5-turbo-0125"

# Criar os PROMPTs:
prompt = ChatPromptTemplate.from_messages(
    [
        (
"system",
"Voc√™ √© um Editor de Conte√∫do que analisa e sumariza documentos. Sua tarefa √© fornecer resumos claros, objetivos e informativos de diferentes tipos de documentos."
" Seu resumo deve ser factual, baseando-se no documento original e abrangendo os aspectos mais importantes dele."
" Fornecerei a voc√™ um documento e voc√™ fonecer√° um resumo de dois par√°grafos desse documento."
" Seja objetivo e evite opini√µes pessoais."
" Se o usu√°rio fizer cr√≠ticas, responda com uma vers√£o melhorada e adicione RESUMO seguido do t√≠tulo original do documento.",
        ),
        MessagesPlaceholder(variable_name="text"),
    ]
)

reflection_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Voc√™ √© um Especialista em Sumariza√ß√£o que analisa e d√° sugest√µes detalhadas para melhorar o resumo fornecido."
            " Revise cuidadosamente a an√°lise do Editor de Conte√∫do e avalie os par√°grafos que precisam a ser melhorados."
            " Seu objetivo √© ajudar ao Editor de Conte√∫do a tornar o resumo mais claro, factual e informativo."
            " Verifique que o Editor de Conte√∫do fa√ßa um resumo curto de apenas dois par√°grafos.",
        ),
        MessagesPlaceholder(variable_name="text"),
    ]
)

# Criar a fun√ß√£o de gera√ß√£o:
generate = prompt | llm
# Crio o objeto de Refletir:
reflect = reflection_prompt | llm

# Criar a fun√ß√£o de gera√ß√£o de n√≥s:
async def generation_node(state: Sequence[BaseMessage]):
    return await generate.ainvoke({"text": state})

# Criar a fun√ß√£o de reflex√£o:
async def reflection_node(messages: Sequence[BaseMessage]) -> List[BaseMessage]:
    # Outras mensagens que precisamos ajustar:
    cls_map = {"ai": HumanMessage, "human": AIMessage}

    # A primeira mensagem √© a solicita√ß√£o original do usu√°rio. Mantemos o mesmo para todos os n√≥s:
    translated = [messages[0]] + [
        cls_map[msg.type](content=msg.content) for msg in messages[1:]
    ]
    res = await reflect.ainvoke({"text": translated})

     # Isso ser√° tratado como um feedback para o gerador:
    return HumanMessage(content=res.content)

# Criar o grafo de mensagens:
builder = MessageGraph()
builder.add_node("generate", generation_node)
builder.add_node("reflect", reflection_node)
builder.set_entry_point("generate")

# Definir a condi√ß√£o para parar a gera√ß√£o:
def should_continue(state: List[BaseMessage]):
    if len(state) > 3:
        return END
    return "reflect"

# Adicionar as condi√ß√µes de parada:
builder.add_conditional_edges("generate", should_continue)
builder.add_edge("reflect", "generate")

# Compilar o grafo:
graph = builder.compile()


"""Aqui come√ßamos a usar o FastAPI:"""

# Endpoint para processar o PDF:
@app.post("/PDF-document-summary/")
async def process_pdf(file: UploadFile = File(...)):
    file_location = f"/tmp/{file.filename}" # Geramos um arquivo tempor√°rio para salvar o PDF
    with open(file_location, "wb") as f:
        f.write(file.file.read())

    text = extract_text_with_langchain_pdf(file_location)

    # Fun√ß√£o para gerar respostas:
    async def stream_responses():
        # last_content = None
        async for event in graph.astream([HumanMessage(content=text)]):
            if 'generate' in event:
                last_content = event['generate'].content # Armazena o 'content' da mensagem 'generate'
        return last_content

    result = await stream_responses()
    return {"Resumo do Documento": result}





if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
