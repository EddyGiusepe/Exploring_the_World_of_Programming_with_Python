{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1 align=\"center\"><font color=\"red\">Agente RAG desenvolvido por LangChain</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"yellow\">Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este Notebook foi baseado no tutorial de [AI Makerspace]():\n",
        "\n",
        "Speakers: \n",
        "\n",
        "[Dr. Greg, Co-Founder & CEO]()\n",
        "\n",
        "[The Wiz, Co-Founder & CTO]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://colab.research.google.com/drive/1_8r0Tj-f2UyRZ4SBuaP-RfhdUeXXxu0Z?usp=sharing#scrollTo=gZudebB0it20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "No Notebook a seguir, daremos uma olhada em como construir `aplicativos Agentic RAG com LangChain`.\n",
        "\n",
        "Contaremos com algumas ferramentas excelentes para nos ajudar a fazer isso:\n",
        "\n",
        "1. LangChain - mais especificamente `LCEL`\n",
        "2. LangGraph\n",
        "3. LangSmith\n",
        "\n",
        "Vamos começar com uma rápida visão geral do `LCEL` e construir uma `Chain RAG simples`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Dependências\n",
        "\n",
        "Primeiro instalaremos todas as nossas bibliotecas necessárias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "2d76b2bf-d145-4f80-d842-23e3bc996622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBvTZ-JshKuo",
        "outputId": "e221e0bd-7d84-46ab-8a47-ad84c7579e6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU faiss-cpu pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Variáveis de ambiente\n",
        "\n",
        "Queremos definir nossa chave de API OpenAI e nossas variáveis ​​de ambiente LangSmith."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "1332fa2d-9c2e-4845-d6be-a9f838977172"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import getpass\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
        "\n",
        "# import os\n",
        "# import getpass\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
        "\n",
        "import openai\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "#openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "#Eddy_key_openai  = os.environ['OPENAI_API_KEY']\n",
        "#from openai import OpenAI\n",
        "#client = OpenAI(api_key=Eddy_key_openai)\n",
        "\n",
        "# Usando LangSmith:\n",
        "LANGCHAIN_TRACING_V2=\"true\"\n",
        "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"  # Eddy ---> https://smith.langchain.com/o/fc23d72c-9360-5a5f-affa-26c44b810011\n",
        "LANGCHAIN_API_KEY=\"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\"\n",
        "LANGCHAIN_PROJECT=\"LangGraph_EddyGiusepe\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "abc68d0e-d2f3-42bb-a2d0-65a5bba3d45d"
      },
      "outputs": [],
      "source": [
        "# from uuid import uuid4\n",
        "\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-CufnKBfFMq"
      },
      "source": [
        "## Initialize a Simple Chain using LCEL\n",
        "\n",
        "The first thing we'll do is familiarize ourselves with LCEL and the specific ins and outs of how we can use it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnuLQnqzg0DZ"
      },
      "source": [
        "### Retrieval\n",
        "\n",
        "First, we'll set up a simple local retriever system that looks at Arxiv papers on the topic of RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g8rnnJbJg8sB"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=3).load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=350,\n",
        "                                                                     chunk_overlap=50\n",
        "                                                                    )\n",
        "\n",
        "chunked_documents = text_splitter.split_documents(docs)\n",
        "\n",
        "faiss_vectorstore = FAISS.from_documents(documents=chunked_documents,\n",
        "                                         embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
        "                                        )\n",
        "\n",
        "retriever = faiss_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-rSWYKjinCA"
      },
      "source": [
        "### Augmented\n",
        "\n",
        "Now that we have our retrieval system ready to rock, we can create our RAG prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gZudebB0it20"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Use o contexto a seguir para responder à consulta do usuário. Se não conseguir responder à pergunta, responda com 'Não sei'.\n",
        "\n",
        "Pergunta:\n",
        "{question}\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPVEVKmPfoYs"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Let's start by initializing our model. In this case, we'll be using OpenAI's `gpt-3.5-turbo` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAU6ilyPfufs"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhxJkDIujhC-"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "Now that we have our R, A, and G components - let's build our simple RAG chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4itzKufj-OI"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_generation_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4vr1n-7kN-y"
      },
      "source": [
        "And let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocoqybsjkPXz",
        "outputId": "227ab541-9f47-4941-a2df-b84f6b7a0c26"
      },
      "outputs": [],
      "source": [
        "await retrieval_augmented_generation_chain.ainvoke({\"question\" : \"What is Retrieval Augmented Generation?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "##  Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "40b6f2cd-540f-42ba-a5c3-e5533d3fb8c9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXGeN-C8mbBM"
      },
      "source": [
        "## Agentic RAG with LangGraph and LCEL\n",
        "\n",
        "Let's see what our final graph will look like:\n",
        "\n",
        "![image](https://i.imgur.com/aL4Qs0E.png)\n",
        "\n",
        "Now that we have our two major components, let's create our final agent!\n",
        "\n",
        "Since we can add any LCEL `Runnable` to our graph as a node directly - we can add our RAG chain as a node with no additional steps!\n",
        "\n",
        "However, since our `Runnable` was set-up without knowledge of our state object - we need to add some pre/post processing steps to ensure it fits into the flow!\n",
        "\n",
        "> NOTE: There is only one cycle in this graph, as we cannot reach the RAG chain after the initial attempt to use it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlCwP-iXojru"
      },
      "outputs": [],
      "source": [
        "def convert_state_to_query(state_object):\n",
        "  return {\"question\" : state_object[\"messages\"][-1].content}\n",
        "\n",
        "def convert_response_to_state(response):\n",
        "  return {\"messages\" : [response[\"response\"]]}\n",
        "\n",
        "langgraph_node_rag_chain = convert_state_to_query | retrieval_augmented_generation_chain | convert_response_to_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTXZtCgouJ2z"
      },
      "source": [
        "Let's test our our new chain and verify it works as expected!\n",
        "\n",
        "> NOTE: We are still able to take advantage of the benefits of built in `async` provided by LCEL with this chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ku8Vk0krNU_",
        "outputId": "0d8d9464-215d-412a-8d02-25f1a7145782"
      },
      "outputs": [],
      "source": [
        "await langgraph_node_rag_chain.ainvoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqz0HD_-uVLq"
      },
      "source": [
        "Now we'll add our nodes - notice that we're including our newly built LCEL component as a node called `first_action`.\n",
        "\n",
        "The basic idea is that we will use our private RAG set-up - and if that is deemed sufficient, we will return that response to our user; and if not we will augment our response will the other tools!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvMuxumArtT7"
      },
      "outputs": [],
      "source": [
        "rag_agent = StateGraph(AgentState)\n",
        "\n",
        "rag_agent.add_node(\"agent\", call_model)\n",
        "rag_agent.add_node(\"action\", call_tool)\n",
        "rag_agent.add_node(\"first_action\", langgraph_node_rag_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaxHeBtsz-JF"
      },
      "source": [
        "Let's set our new entry point to be our RAG pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpHI3VhDr72t"
      },
      "outputs": [],
      "source": [
        "rag_agent.set_entry_point(\"first_action\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-GFjQen0BeH"
      },
      "source": [
        "Because we wish to have this conditional behaviour (\"is the question fully answered by the RAG pipeline?\") we'll need to add a new conditional node!\n",
        "\n",
        "We'll start by describing a process by which we can ask the question: \"Is this question fully answered by the response?\"\n",
        "\n",
        "This will let us boil down our paths to \"Yes, it is fully answered\", and \"No, it is not fully answered\".\n",
        "\n",
        "The function below will do exactly that by leaning on Pydantic and GPT-4!\n",
        "\n",
        "> NOTE: We now have an LCEL component as a node, and we have a chain *inside a function in as a node*. LangGraph is an extremely flexible framework!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-QZVNURvDTe"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "\n",
        "def is_fully_answered(state):\n",
        "\n",
        "  ### Extract the question and response from our RAG pipeline\n",
        "  question = state[\"messages\"][0].content\n",
        "  answer = state[\"messages\"][-1].content\n",
        "\n",
        "  ### Create a Pydantic model to capture our LLMs response\n",
        "  class answered(BaseModel):\n",
        "    binary_score: str = Field(description=\"Fully answered: 'yes' or 'no'\")\n",
        "\n",
        "  ### A powerful reasoning model will ensure we can answer our question properly\n",
        "  model = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0)\n",
        "\n",
        "  ### Create and bind our tool to our model\n",
        "  answered_tool = convert_to_openai_tool(answered)\n",
        "\n",
        "  model = model.bind(\n",
        "      tools=[answered_tool],\n",
        "      tool_choice={\"type\" : \"function\", \"function\" : {\"name\" : \"answered\"}}\n",
        "  )\n",
        "\n",
        "  ### We'll want to parse the output into a usable format\n",
        "  parser_tool = PydanticToolsParser(tools=[answered])\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"\"\"You will determine if the question is fully answered by the response.\\n\n",
        "      Question:\n",
        "      {question}\n",
        "\n",
        "      Response:\n",
        "      {answer}\n",
        "\n",
        "      You will respond with either 'yes' or 'no'.\"\"\",\n",
        "      input_variables=[\"question\", \"answer\"])\n",
        "\n",
        "  ### Classic LCEL chain!\n",
        "  fully_answered_chain = prompt | model | parser_tool\n",
        "\n",
        "  response = fully_answered_chain.invoke({\"question\" : question, \"answer\" : answer})\n",
        "\n",
        "  if response[0].binary_score == \"no\":\n",
        "    return \"continue\"\n",
        "\n",
        "  return \"end\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeG_LWxn1PJJ"
      },
      "source": [
        "Let's map and add that conditional edge now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R83jwrx8xKVf"
      },
      "outputs": [],
      "source": [
        "rag_agent.add_conditional_edges(\n",
        "    \"first_action\",\n",
        "    is_fully_answered,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFyVeahl1fdj"
      },
      "source": [
        "We'll still use our original prompt to determine if we need to use more tools or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD8wC2W6r_tb"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "rag_agent.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1BR2VMW1k7E"
      },
      "source": [
        "Let's define the final edge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvQzFbtJsDrE"
      },
      "outputs": [],
      "source": [
        "rag_agent.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_8ZHvch1mqB"
      },
      "source": [
        "Time to compile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bgho8n0QsO_s"
      },
      "outputs": [],
      "source": [
        "rag_agent_app = rag_agent.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu069aIF1pAU"
      },
      "source": [
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sItf-jgY1rMU",
        "outputId": "ddac59ca-111a-42c8-d03b-e7ee033a98f5"
      },
      "outputs": [],
      "source": [
        "question = \"Who is the main author on the Retrieval Augmented Generation paper?\"\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=question)]}\n",
        "\n",
        "rag_agent_app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHj-i2Cc1uud"
      },
      "source": [
        "Notice how we didn't enter into our tool cycle as the query was fully answered by our baseline RAG system!\n",
        "\n",
        "Let's try another example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzQY4SSvsSE1",
        "outputId": "726c8b20-c965-447a-d99c-f3ddcd91a853"
      },
      "outputs": [],
      "source": [
        "question = \"Who is the main author on the Retrieval Augmented Generation paper - and what University did they attend?\"\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=question)]}\n",
        "\n",
        "rag_agent_app.invoke(inputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
