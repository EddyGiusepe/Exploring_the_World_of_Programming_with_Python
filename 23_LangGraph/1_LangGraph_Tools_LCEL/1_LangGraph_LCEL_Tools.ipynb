{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1 align=\"center\"><font color=\"red\">LangGraph: Construindo Aplicações Cíclicas com LangChain</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"yellow\">Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este Notebook foi baseado no tutorial de [AI Makerspace]():\n",
        "\n",
        "Speakers: \n",
        "\n",
        "[Dr. Greg, Co-Founder & CEO]()\n",
        "\n",
        "[The Wiz, Co-Founder & CTO]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "</font color=\"orange\">LangGraph é uma ferramenta que aproveita a linguagem de expressão LangChain para construir aplicativos coordenados com vários atores e com estado que incluem comportamento cíclico.</font>\n",
        "\n",
        "## Por que Ciclos?\n",
        "\n",
        "Em essência, podemos pensar em um ciclo em nosso `Graph` como um loop mais robusto e personalizável. Isso nos permite manter nosso aplicativo avançado enquanto ainda oferece a poderosa funcionalidade dos loops tradicionais.\n",
        "\n",
        "Devido à inclusão de ciclos sobre loops, também podemos compor fluxos bastante complexos através do nosso `Graph` de uma forma muito mais legível e natural. Permitindo-nos efetivamente recriar fluxogramas de aplicativos em código de uma forma quase `1-to-1`.\n",
        "\n",
        "## Por que LangGraph?\n",
        "\n",
        "Além da abordagem de agente direto - podemos facilmente compor e combinar cadeias tradicionais `\"DAG\"` (Directed Acyclic Graph) com comportamento cíclico poderoso devido à forte integração com `LCEL` ([LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/)). Isso significa que é uma extensão natural das principais ofertas da `LangChain`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Dependências\n",
        "\n",
        "Pegaremos algumas dependências e adicionaremos algumas variáveis ​​de ambiente que aproveitaremos em todo o notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "39ca9840-6772-45f3-912e-853e79095a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "seaplane-cli 0.0.22b1 requires click==8.1.3, but you have click 8.1.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "8e6e9d5d-2dc9-4290-dc6e-7e28cbc77e9e"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import getpass\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
        "\n",
        "import openai\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "#openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "#Eddy_key_openai  = os.environ['OPENAI_API_KEY']\n",
        "#from openai import OpenAI\n",
        "#client = OpenAI(api_key=Eddy_key_openai)\n",
        "\n",
        "# Usando LangSmith:\n",
        "LANGCHAIN_TRACING_V2=\"true\"\n",
        "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"  # Eddy ---> https://smith.langchain.com/o/fc23d72c-9360-5a5f-affa-26c44b810011\n",
        "LANGCHAIN_API_KEY=\"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\"\n",
        "LANGCHAIN_PROJECT=\"LangGraph_EddyGiusepe\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "a4f4e67a-473a-40e9-9a9e-3639c6bddd27"
      },
      "outputs": [],
      "source": [
        "# from uuid import uuid4\n",
        "\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangGraph Demo - {uuid4().hex[0:8]}\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Tool Selection\n",
        "\n",
        "### Creating the Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [DuckDuckGoSearchRun(), ArxivQueryRun()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, streaming=True) # # \"gpt-4o\"   ou    \"gpt-3.5-turbo-0125\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'duckduckgo_search',\n",
              "  'description': 'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'query': {'description': 'search query to look up',\n",
              "     'type': 'string'}},\n",
              "   'required': ['query']}},\n",
              " {'name': 'arxiv',\n",
              "  'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'query': {'description': 'search query to look up',\n",
              "     'type': 'string'}},\n",
              "   'required': ['query']}}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d646c2e1780>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d646c2e2e90>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', streaming=True), kwargs={'functions': [{'name': 'duckduckgo_search', 'description': 'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}, {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}]})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "async def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = await model.ainvoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "async def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = await tool_executor.ainvoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\"agent\",\n",
        "                               should_continue,\n",
        "                               {\n",
        "                                  \"continue\" : \"action\",\n",
        "                                  \"end\" : END\n",
        "                               }\n",
        "                              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "3242cfc8-5db2-45e8-90c1-66394cf4258b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='O que são mutantes de nível ômega? Forneça fontes em sua resposta.'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"O que são mutantes de nível ômega\"}', 'name': 'duckduckgo_search'}}, response_metadata={'finish_reason': 'function_call'}, id='run-17a446bb-3e80-4915-a404-34b94ab69f03-0'),\n",
              "  FunctionMessage(content='Dentro das HQs da Marvel, os mutantes são colocados numa classificação de acordo com seu nível de poder e o tipo de suas habilidades. Esse sistema possui seis classes distintas: Ômega, Alfa, Beta, Gama, Delta e Épsilon, todos nomeados de acordo com letras do alfabeto grego. A ideia por trás disso é mostrar como as mutações podem ser ... Caso você esteja acompanhando os eventos de X-Men \\'97, já se deparou com alguns momentos da animação em que mutantes como Tempestade são classificados como pertencentes ao \"nível Ômega ... Tempo estimado de leitura: 11 minutos O Universo Marvel possui mutantes extremamente poderosos, capazes de feitos incríveis. Ao longo dos anos, os mutantes mais fortes dos X-Men foram categorizados como \"nível ômega\", mas nem sempre foram classificados oficialmente. A reformulação dos títulos dos X-Men por Jonathan Hickman no que ficou conhecido como \"Era Krakoa\" revelou finalmente uma ... Entenda o que é o nível Ômega e saiba quais são os mutantes de X-Men 97 que possuem esse poder. ... Mutantes de nível ômega pertencem são os mais poderosos entre os mutantes de X-Men 97. Estes personagens não têm limitação dentro do campo dos seus próprios poderes. Dependendo da habilidade dos heróis ou vilões, eles podem até ... Ao longo de múltiplas adaptações e continuidades, vários X-Men filmes e programas de TV introduziram mutantes no nível Omega.Os heróis e vilões mutantes apresentados no X-Men os quadrinhos são considerados alguns dos personagens mais poderosos da Marvel. No entanto, mesmo entre os mutantes da Marvel, existem hierarquias de poder, com a força das habilidades dos mutantes classificadas ...', name='duckduckgo_search'),\n",
              "  AIMessage(content='Dentro das HQs da Marvel, os mutantes são classificados de acordo com seu nível de poder e o tipo de suas habilidades. Essa classificação inclui seis classes distintas: Ômega, Alfa, Beta, Gama, Delta e Épsilon, nomeadas de acordo com letras do alfabeto grego. Os mutantes de nível Ômega são considerados os mais poderosos entre os mutantes e não têm limitações dentro do campo de seus próprios poderes.\\n\\nOs mutantes de nível Ômega são capazes de feitos incríveis e são os mais fortes dos X-Men. Eles não possuem limitações em relação às suas habilidades e podem realizar feitos extraordinários. Esses mutantes são considerados alguns dos personagens mais poderosos da Marvel.\\n\\nFonte: [Clique aqui para ler mais](https://www.aficionados.com.br/mutantes-de-nivel-omega-x-men-97/)', response_metadata={'finish_reason': 'stop'}, id='run-1549a9a8-e570-4a66-aef4-ab8789b3aa58-0')]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"O que são mutantes de nível ômega? Forneça fontes em sua resposta.\")]} # \"What are Omega Level Mutants? Provide sources in your response.\"\n",
        "\n",
        "await app.ainvoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "9de3d951-5f02-4822-8bcd-d2b5c5cfbd09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='O que é QLoRA em aprendizado de máquina? Há algum documento que possa me ajudar a entender? Depois de ter essas informações, você pode consultar a biografia do primeiro autor do artigo QLoRA?'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA machine learning\"}', 'name': 'arxiv'}}, response_metadata={'finish_reason': 'function_call'}, id='run-27cfdf99-957d-44d1-a90d-2e518c6e3da0-0'),\n",
              "  FunctionMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-02-16\\nTitle: QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\\nAuthors: Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh\\nSummary: Finetuning large language models requires huge GPU memory, restricting the\\nchoice to ac', name='arxiv'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Tim Dettmers\"}', 'name': 'duckduckgo_search'}}, response_metadata={'finish_reason': 'function_call'}, id='run-b0888d2b-5697-43a6-a639-7c08e985b629-0'),\n",
              "  FunctionMessage(content='Sponsored by Evolution AI: https://www.evolution.aiAbstract: Recent open-source large language models (LLMs) like LLaMA and Falcon are both high-quality and ... Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required — from over 780GB to less than 48GB — to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... Episode 82 of the Stanford MLSys Seminar Series!Democratizing Foundation Models via k-bit QuantizationSpeaker: Tim DettmersAbstract:Foundation models are eff... If you have a curiosity about how fancy graphics cards actually work, and why they are so well-suited to AI-type applications, then take a few minutes to read [Tim Dettmers] explain why this is so.… Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public. QLoRA: One of the core contributions of bitsandbytes towards the democratization of AI.', name='duckduckgo_search'),\n",
              "  AIMessage(content='O artigo sobre QLoRA em aprendizado de máquina é intitulado \"QLoRA: Efficient Finetuning of Quantized LLMs\" e foi escrito por Tim Dettmers, Artidoro Pagnoni, Ari Holtzman e Luke Zettlemoyer. O artigo apresenta uma abordagem eficiente de ajuste fino que reduz o uso de memória o suficiente para ajustar um modelo de 65 bilhões de parâmetros em uma única GPU de 48GB, preservando o desempenho da tarefa de ajuste fino de 16 bits. O artigo detalha várias inovações introduzidas pelo QLoRA para economizar memória sem sacrificar o desempenho, incluindo um novo tipo de dados chamado NormalFloat de 4 bits (NF4) e otimizadores paginados para gerenciar picos de memória.\\n\\nO primeiro autor do artigo é Tim Dettmers. Ele é um acadêmico especializado em quantização e uso de aceleradores de hardware de aprendizado profundo. Se desejar mais informações sobre Tim Dettmers, você pode acessar o site da Evolution AI: [Tim Dettmers - Evolution AI](https://www.evolution.ai).', response_metadata={'finish_reason': 'stop'}, id='run-9dda5617-501a-41d7-8499-f163ea7028d6-0')]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# \"What is QLoRA in Machine Learning? Are their any papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\"\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"O que é QLoRA em aprendizado de máquina? Há algum documento que possa me ajudar a entender? Depois de ter essas informações, você pode consultar a biografia do primeiro autor do artigo QLoRA?\")]}\n",
        "\n",
        "await app.ainvoke(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
