{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"><font color=\"red\">How to Build a Simple Reasoning and Acting Agent from Scratch</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este Notebook está baseado no Tutorial de [kirouane Ayoub]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links de estudo:\n",
    "\n",
    "* [Part 1](https://blog.gopenai.com/introduction-to-llm-agents-how-to-build-a-simple-reasoning-and-acting-agent-from-scratch-part-1-843e14686be7)\n",
    "\n",
    "* [Part 2](https://blog.gopenai.com/building-llm-agents-from-scratch-part-2-a-conversational-search-agent-with-ollama-a4544b2291cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Contextualizando</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta Notebook inicia uma série sobre a construção de `agentes de IA´, começando com uma implementação básica usando `Ollama´ para executar `LLMs´ localmente. O objetivo é criar um sistema que compreenda consultas do usuário e utilize pesquisa na web para fornecer respostas informativas, demonstrando os conceitos fundamentais do design de agentes de IA.\n",
    "\n",
    "Esta implementação serve como um ponto de partida para entender o desenvolvimento de `agentes de IA`, mas é uma versão simplificada. Em cenários reais, os agentes teriam comportamentos mais complexos, exigindo técnicas de análise sintática robustas para lidar com saídas inesperadas. Além disso, seriam necessárias estratégias abrangentes de tratamento de erros para garantir uma operação suave e confiável do sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Funcionamento do Agent</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*-c8XfC8zGlrmF8DMwWkJaA.png\" alt=\"Minha Imagem\" width=\"700\" height=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Agent:` Representa o sistema geral que abrange o LLM e suas interações com o ambiente.\n",
    "\n",
    "* `Task:` Este é o objetivo ou meta que o agente está tentando atingir. É o ponto de partida do processo.\n",
    "\n",
    "* `LLM:` Este é o núcleo do agente, um LLM que processa informações e toma decisões.\n",
    "\n",
    "* `Reasoning:` O LLM utiliza o raciocínio para determinar o melhor curso de ação com base na tarefa e nas informações disponíveis.\n",
    "\n",
    "* `Tools:` O agente pode utilizar várias ferramentas (`como pesquisa na web`, `calculadoras` ou `bancos de dados`) para coletar informações adicionais ou executar ações.\n",
    "\n",
    "* `Action:` Com base em seu raciocínio, o LLM decide uma ação a ser tomada em seu ambiente.\n",
    "\n",
    "* `Environment:` Representa o mundo externo ou contexto no qual o agente opera. `Pode ser um site`, `um espaço físico` ou `um ambiente simulado`.\n",
    "\n",
    "* `Result:` A ação tomada pelo agente produz um resultado no ambiente, que então realimenta a compreensão e as ações futuras do agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Exemplo de código</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Configurando Ollama e Dependências</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "ollama serve\n",
    "ollama pull llama3.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também:\n",
    "\n",
    "```\n",
    "pip install openai duckduckgo-search\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Criando um cliente OpenAI para interação LLM local</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "Eddy_key_openai  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=Eddy_key_openai,\n",
    "                base_url=\"http://127.0.0.1:11434/v1\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Implementando a funcionalidade de pesquisa</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso agente precisa da capacidade de pesquisar informações na `web`. Definimos uma função `search` que utiliza a biblioteca `duckduckgo-search` para consultar `DuckDuckGo` e recuperar resultados de pesquisa relevantes.\n",
    "\n",
    "\n",
    "\n",
    "`Esses resultados serão usados ​​para aumentar o conhecimento do LLM quando necessário.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "def search(query , max_results=3):\n",
    "  results = DDGS().text(query, max_results=max_results)\n",
    "\n",
    "  return str(\"\\n\".join(str(results[i]) for i in range(len(results))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Definindo ação e extração de entrada</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para orientar o comportamento do agente, precisamos entender suas ações pretendidas com base na saída do `LLM`.\n",
    "\n",
    "\n",
    "Definimos uma função para extrair a `“Action”` e a `“Action Input”` da resposta do `LLM`, permitindo-nos determinar se ele pretende responder diretamente ao usuário ou realizar uma pesquisa na web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_action_and_input(text):\n",
    "  action_pattern = r\"Action: (.+?)\\n\"\n",
    "  input_pattern = r\"Action Input: \\\"(.+?)\\\"\"\n",
    "  action = re.findall(action_pattern, text)\n",
    "  action_input = re.findall(input_pattern, text)\n",
    "  return action, action_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Elaborando o Prompt do Sistema</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O prompt do sistema (`system prompt`) fornece instruções ao `LLM` sobre como se comportar. Ele descreve as ferramentas disponíveis (`neste caso, pesquisa na web`) e o processo de tomada de decisão para escolher entre respostas diretas e utilizar a ferramenta de pesquisa.\n",
    "\n",
    "\n",
    "`Este prompt garante que o agente siga um fluxo de trabalho lógico.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "System_prompt = \"\"\"\n",
    "Responda às seguintes perguntas da melhor forma possível. Use a pesquisa na web somente se necessário. \n",
    "\n",
    "Você tem acesso à seguinte ferramenta: \n",
    "\n",
    "Pesquisa: útil para quando você precisa responder a perguntas sobre eventos atuais ou quando precisa de informações específicas que não conhece. \n",
    "          No entanto, você só deve usá-la se a consulta do usuário não puder ser respondida com conhecimento geral ou uma resposta direta. \n",
    "\n",
    "Você receberá uma mensagem do humano, então você deve iniciar um loop e fazer um dos seguintes: \n",
    "\n",
    "Opção 1: Responder ao humano diretamente \n",
    "         - Se a pergunta do usuário for simples, coloquial ou puder ser respondida com seu conhecimento existente, responda diretamente. \n",
    "         - Se o usuário não solicitou uma pesquisa ou a pergunta não precisa de informações atualizadas ou específicas, evite usar a ferramenta de pesquisa. \n",
    "\n",
    "Use o seguinte formato ao responder ao humano: \n",
    "Pensamento: Explique por que uma resposta direta é suficiente ou por que nenhuma pesquisa é necessária. \n",
    "Ação: Resposta ao Humano\n",
    "Entrada de ação: \"sua resposta ao humano, resumindo o que você sabe ou concluindo a conversa\" \n",
    "\n",
    "Opção 2: Use a ferramenta de pesquisa para responder à pergunta. \n",
    "         - Se precisar de mais informações para responder à pergunta, ou se a pergunta for sobre um evento atual ou conhecimento específico que pode não estar nos seus dados de treinamento, use a ferramenta de pesquisa. \n",
    "         - Após receber os resultados da pesquisa, decida se você tem informações suficientes para responder à pergunta ou se outra pesquisa é necessária. \n",
    "\n",
    "Use o seguinte formato ao usar a ferramenta de pesquisa: \n",
    "Pensamento: Explique por que uma pesquisa é necessária ou por que pesquisas adicionais são necessárias. \n",
    "Ação: Pesquisar \n",
    "ação Entrada: \"a entrada para a ação, a ser enviada para a ferramenta\" \n",
    "\n",
    "Lembre-se de sempre decidir cuidadosamente se deve pesquisar ou responder diretamente. Se puder responder à pergunta sem pesquisar, sempre prefira responder diretamente. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_All",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
