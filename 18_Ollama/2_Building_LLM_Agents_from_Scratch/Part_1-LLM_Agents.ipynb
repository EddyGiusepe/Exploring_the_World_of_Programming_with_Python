{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"><font color=\"red\">How to Build a Simple Reasoning and Acting Agent from Scratch</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este Notebook está baseado no Tutorial de [kirouane Ayoub]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links de estudo:\n",
    "\n",
    "* [Part 1](https://blog.gopenai.com/introduction-to-llm-agents-how-to-build-a-simple-reasoning-and-acting-agent-from-scratch-part-1-843e14686be7)\n",
    "\n",
    "* [Part 2](https://blog.gopenai.com/building-llm-agents-from-scratch-part-2-a-conversational-search-agent-with-ollama-a4544b2291cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Contextualizando</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta Notebook inicia uma série sobre a construção de `agentes de IA´, começando com uma implementação básica usando `Ollama´ para executar `LLMs´ localmente. O objetivo é criar um sistema que compreenda consultas do usuário e utilize pesquisa na web para fornecer respostas informativas, demonstrando os conceitos fundamentais do design de agentes de IA.\n",
    "\n",
    "Esta implementação serve como um ponto de partida para entender o desenvolvimento de `agentes de IA`, mas é uma versão simplificada. Em cenários reais, os agentes teriam comportamentos mais complexos, exigindo técnicas de análise sintática robustas para lidar com saídas inesperadas. Além disso, seriam necessárias estratégias abrangentes de tratamento de erros para garantir uma operação suave e confiável do sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Funcionamento do Agent</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*-c8XfC8zGlrmF8DMwWkJaA.png\" alt=\"Minha Imagem\" width=\"700\" height=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Agent:` Representa o sistema geral que abrange o LLM e suas interações com o ambiente.\n",
    "\n",
    "* `Task:` Este é o objetivo ou meta que o agente está tentando atingir. É o ponto de partida do processo.\n",
    "\n",
    "* `LLM:` Este é o núcleo do agente, um LLM que processa informações e toma decisões.\n",
    "\n",
    "* `Reasoning:` O LLM utiliza o raciocínio para determinar o melhor curso de ação com base na tarefa e nas informações disponíveis.\n",
    "\n",
    "* `Tools:` O agente pode utilizar várias ferramentas (`como pesquisa na web`, `calculadoras` ou `bancos de dados`) para coletar informações adicionais ou executar ações.\n",
    "\n",
    "* `Action:` Com base em seu raciocínio, o LLM decide uma ação a ser tomada em seu ambiente.\n",
    "\n",
    "* `Environment:` Representa o mundo externo ou contexto no qual o agente opera. `Pode ser um site`, `um espaço físico` ou `um ambiente simulado`.\n",
    "\n",
    "* `Result:` A ação tomada pelo agente produz um resultado no ambiente, que então realimenta a compreensão e as ações futuras do agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Exemplo de código</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Configurando Ollama e Dependências</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "ollama serve\n",
    "ollama pull llama3.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também:\n",
    "\n",
    "```\n",
    "pip install openai duckduckgo-search\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Criando um cliente OpenAI para interação LLM local</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "Eddy_key_openai  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=Eddy_key_openai,\n",
    "                base_url=\"http://127.0.0.1:11434/v1\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Implementando a funcionalidade de pesquisa</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso agente precisa da capacidade de pesquisar informações na `web`. Definimos uma função `search` que utiliza a biblioteca `duckduckgo-search` para consultar `DuckDuckGo` e recuperar resultados de pesquisa relevantes.\n",
    "\n",
    "\n",
    "\n",
    "`Esses resultados serão usados ​​para aumentar o conhecimento do LLM quando necessário.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from duckduckgo_search import DDGS\n",
    "\n",
    "# def search(query , max_results=10):\n",
    "#   results = DDGS().text(query, max_results=max_results)\n",
    "#   return str(\"\\n\".join(str(results[i]) for i in range(len(results))))\n",
    "\n",
    "\n",
    "from duckduckgo_search import DDGS\n",
    "import logging\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def search(query , max_results=3):\n",
    "    \"\"\"\n",
    "    Realiza uma pesquisa web usando DuckDuckGo e retorna os resultados.\n",
    "    \n",
    "    Args:\n",
    "        query (str): A consulta de pesquisa.\n",
    "        max_results (int): O número máximo de resultados a retornar.\n",
    "    \n",
    "    Returns:\n",
    "        str: Uma string com os resultados da pesquisa.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Realizando pesquisa na Internet para: {query}\")\n",
    "    try:\n",
    "        results = DDGS().text(query, max_results=max_results)\n",
    "        logging.info(\"Research completed.\")\n",
    "        return str(\"\\n\".join(str(results[i]) for i in range(len(results))))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during search: {str(e)}\")\n",
    "        return f\"Error during search: {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Press release: The Nobel Prize in Physics 2024 - NobelPrize.org', 'href': 'https://www.nobelprize.org/prizes/physics/2024/press-release/', 'body': '8 October 2024. The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Physics 2024 to. John J. Hopfield. Princeton University, NJ, USA. Geoffrey E. Hinton. University of Toronto, Canada. \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\".'}\n",
      "{'title': 'Nobel Prize in physics 2024 awarded for work on artificial intelligence ...', 'href': 'https://www.cnn.com/2024/10/08/science/nobel-prize-physics-hopfield-hinton-machine-learning-intl/index.html', 'body': 'The 2024 Nobel Prize in physics has been awarded to John Hopfield and Geoffrey Hinton for their fundamental discoveries in machine learning, which paved the way for how artificial intelligence is ...'}\n"
     ]
    }
   ],
   "source": [
    "# Testando o Duck:\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Inicializa a pesquisa:\n",
    "with DDGS() as ddgs:\n",
    "    # Define a consulta de pesquisa:\n",
    "    query = \"Who won the 2024 Nobel Prize in Physics?\"\n",
    "    \n",
    "    # Realiza a pesquisa e limita os resultados a 2:\n",
    "    results = [r for r in ddgs.text(query, max_results=2)]\n",
    "    \n",
    "    # Exibe os resultados\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Definindo ação e extração de entrada</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para orientar o comportamento do agente, precisamos entender suas ações pretendidas com base na saída do `LLM`.\n",
    "\n",
    "\n",
    "Definimos uma função para extrair a `“Action”` e a `“Action Input”` da resposta do `LLM`, permitindo-nos determinar se ele pretende responder diretamente ao usuário ou realizar uma pesquisa na web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_action_and_input(text):\n",
    "  action_pattern = r\"Action: (.+?)\\n\"\n",
    "  input_pattern = r\"Action Input: \\\"(.+?)\\\"\"\n",
    "  action = re.findall(action_pattern, text)\n",
    "  action_input = re.findall(input_pattern, text)\n",
    "  logging.info(f\"Action extracted: {action}, Action input: {action_input}\")\n",
    "  return action, action_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Elaborando o Prompt do Sistema</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O prompt do sistema (`system prompt`) fornece instruções ao `LLM` sobre como se comportar. Ele descreve as ferramentas disponíveis (`neste caso, pesquisa na web`) e o processo de tomada de decisão para escolher entre respostas diretas e utilizar a ferramenta de pesquisa.\n",
    "\n",
    "\n",
    "`Este prompt garante que o agente siga um fluxo de trabalho lógico.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "System_prompt = \"\"\"\n",
    "Answer the following questions as best you can. Use web search only if necessary.\n",
    "\n",
    "You have access to the following tool:\n",
    "\n",
    "Search: useful for when you need to answer questions about current events or when you need specific information that you don't know. However, you should only use it if the user's query cannot be answered with general knowledge or a straightforward response.\n",
    "\n",
    "You will receive a message from the human, then you should start a loop and do one of the following:\n",
    "\n",
    "Option 1: Respond to the human directly\n",
    "- If the user's question is simple, conversational, or can be answered with your existing knowledge, respond directly.\n",
    "- If the user did not ask for a search or the question does not need updated or specific information, avoid using the search tool.\n",
    "\n",
    "Use the following format when responding to the human:\n",
    "Thought: Explain why a direct response is sufficient or why no search is needed.\n",
    "Action: Response To Human\n",
    "Action Input: \"your response to the human, summarizing what you know or concluding the conversation\"\n",
    "\n",
    "Option 2: Use the search tool to answer the question.\n",
    "- If you need more information to answer the question, or if the question is about a current event or specific knowledge that may not be in your training data, use the search tool.\n",
    "- After receiving search results, decide whether you have enough information to answer the question or if another search is necessary.\n",
    "\n",
    "Use the following format when using the search tool:\n",
    "Thought: Explain why a search is needed or why additional searches are needed.\n",
    "Action: Search\n",
    "Action Input: \"the input to the action, to be sent to the tool\"\n",
    "\n",
    "Remember to always decide carefully whether to search or respond directly. If you can answer the question without searching, always prefer responding directly.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Construindo o Loop de Execução do Agente</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O núcleo (`core`) do nosso agente está na função `run_agent`. Esta função gerencia o loop de interação entre o `usuário`, o `LLM` e a `ferramenta de busca`.\n",
    "\n",
    "\n",
    "`Ele processa os prompts do usuário, os envia ao LLM, interpreta a resposta do LLM, realiza pesquisas quando necessário e, por fim, fornece uma resposta final ao usuário.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(prompt, system_prompt):\n",
    "    # Prepare the initial message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    previous_searches = set()  # Track previous search queries to avoid repetition\n",
    "\n",
    "    while True:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3.1:8b\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        response_text = response.choices[0].message.content\n",
    "        #print(\"🤗\", response_text, \"🤗\")\n",
    "\n",
    "        action, action_input = extract_action_and_input(response_text)\n",
    "        if not action or not action_input:\n",
    "            return \"Failed to parse action or action input.\"\n",
    "\n",
    "        if action[-1] == \"Search\":\n",
    "            search_query = action_input[-1].strip()\n",
    "\n",
    "            # Verifique se a consulta de pesquisa já foi realizada antes:\n",
    "            if search_query in previous_searches:\n",
    "                print(\"Repeated search detected. Stopping to prevent infinite loop.\")\n",
    "                break\n",
    "\n",
    "            print(f\"================ Performing search for: {search_query} ================\")\n",
    "            observation = search(search_query)\n",
    "            print(\"================ Search completed !! ================\")\n",
    "            previous_searches.add(search_query)  # Adicionar ao conjunto de pesquisas realizadas\n",
    "\n",
    "\n",
    "            messages.extend([\n",
    "            { \"role\": \"system\", \"content\": response_text },\n",
    "            { \"role\": \"user\", \"content\": f\"Observation: {observation}\" },\n",
    "            ])\n",
    "\n",
    "        elif action[-1] == \"Response To Human\":\n",
    "            print(f\"Response: {action_input[-1]}\")\n",
    "            break\n",
    "\n",
    "        # Evitar loop infinito:\n",
    "        if len(previous_searches) > 3:\n",
    "            print(\"Too many searches performed. Ending to prevent infinite loop.\")\n",
    "            break\n",
    "\n",
    "    return action_input[-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Exemplos ilustrativos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, demonstramos as capacidades do `agente` com dois exemplos.\n",
    "\n",
    "O primeiro mostra uma interação de conversação simples em que o agente responde diretamente a uma saudação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:10,946 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:10,947 - INFO - Action extracted: ['Response To Human'], Action input: ['Hello! Nice to meet you!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Hello! Nice to meet you!\n"
     ]
    }
   ],
   "source": [
    "response = run_agent(prompt=\"Hello!!\",\n",
    "          system_prompt=System_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Nice to meet you!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">O segundo exemplo demonstra um cenário em que o `agente` precisa realizar uma pesquisa na web para responder a uma pergunta sobre a estrutura `Ollama`, destacando sua capacidade de aproveitar `fontes externas de conhecimento`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:28,381 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:28,382 - INFO - Action extracted: ['Search'], Action input: ['ollama framework']\n",
      "2024-10-13 16:02:28,382 - INFO - Realizando pesquisa na Internet para: ollama framework\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Performing search for: ollama framework ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:29,402 - INFO - Research completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Search completed !! ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:33,327 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:33,328 - INFO - Action extracted: ['Response To Human'], Action input: ['The ollama framework is a lightweight, extensible framework for building and running language models on a local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in various applications.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The ollama framework is a lightweight, extensible framework for building and running language models on a local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in various applications.\n"
     ]
    }
   ],
   "source": [
    "response = run_agent(prompt=\"Tell me about the ollama framework\",\n",
    "          system_prompt=System_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ollama framework is a lightweight, extensible framework for building and running language models on a local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in various applications.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:15,759 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:15,760 - INFO - Action extracted: ['Search'], Action input: ['the winners of the 2024 Nobel Prize in Physics']\n",
      "2024-10-13 16:02:15,760 - INFO - Realizando pesquisa na Internet para: the winners of the 2024 Nobel Prize in Physics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Performing search for: the winners of the 2024 Nobel Prize in Physics ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:17,133 - INFO - Research completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Search completed !! ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:19,777 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:19,778 - INFO - Action extracted: ['Respond To Human'], Action input: ['The winners of the 2024 Nobel Prize in Physics were John J. Hopfield and Geoffrey E. Hinton.']\n",
      "2024-10-13 16:02:22,381 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:22,382 - INFO - Action extracted: ['Response To Human'], Action input: ['The winners of the 2024 Nobel Prize in Physics were John J. Hopfield from Princeton University, USA, and Geoffrey E. Hinton from the University of Toronto, Canada. They were awarded for their foundational discoveries and inventions that enabled machine learning with artificial neural networks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The winners of the 2024 Nobel Prize in Physics were John J. Hopfield from Princeton University, USA, and Geoffrey E. Hinton from the University of Toronto, Canada. They were awarded for their foundational discoveries and inventions that enabled machine learning with artificial neural networks.\n"
     ]
    }
   ],
   "source": [
    "response = run_agent(prompt=\"Who were the winners of the 2024 Nobel Prize in Physics?\",\n",
    "          system_prompt=System_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winners of the 2024 Nobel Prize in Physics were John J. Hopfield from Princeton University, USA, and Geoffrey E. Hinton from the University of Toronto, Canada. They were awarded for their foundational discoveries and inventions that enabled machine learning with artificial neural networks.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_All",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
