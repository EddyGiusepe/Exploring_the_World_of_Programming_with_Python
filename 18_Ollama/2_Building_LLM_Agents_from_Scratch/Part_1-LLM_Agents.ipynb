{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"><font color=\"red\">How to Build a Simple Reasoning and Acting Agent from Scratch</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este Notebook está baseado no Tutorial de [kirouane Ayoub]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links de estudo:\n",
    "\n",
    "* [Part 1](https://blog.gopenai.com/introduction-to-llm-agents-how-to-build-a-simple-reasoning-and-acting-agent-from-scratch-part-1-843e14686be7)\n",
    "\n",
    "* [Part 2](https://blog.gopenai.com/building-llm-agents-from-scratch-part-2-a-conversational-search-agent-with-ollama-a4544b2291cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Contextualizando</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta Notebook inicia uma série sobre a construção de `agentes de IA´, começando com uma implementação básica usando `Ollama´ para executar `LLMs´ localmente. O objetivo é criar um sistema que compreenda consultas do usuário e utilize pesquisa na web para fornecer respostas informativas, demonstrando os conceitos fundamentais do design de agentes de IA.\n",
    "\n",
    "Esta implementação serve como um ponto de partida para entender o desenvolvimento de `agentes de IA`, mas é uma versão simplificada. Em cenários reais, os agentes teriam comportamentos mais complexos, exigindo técnicas de análise sintática robustas para lidar com saídas inesperadas. Além disso, seriam necessárias estratégias abrangentes de tratamento de erros para garantir uma operação suave e confiável do sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Funcionamento do Agent</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*-c8XfC8zGlrmF8DMwWkJaA.png\" alt=\"Minha Imagem\" width=\"700\" height=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Agent:` Representa o sistema geral que abrange o LLM e suas interações com o ambiente.\n",
    "\n",
    "* `Task:` Este é o objetivo ou meta que o agente está tentando atingir. É o ponto de partida do processo.\n",
    "\n",
    "* `LLM:` Este é o núcleo do agente, um LLM que processa informações e toma decisões.\n",
    "\n",
    "* `Reasoning:` O LLM utiliza o raciocínio para determinar o melhor curso de ação com base na tarefa e nas informações disponíveis.\n",
    "\n",
    "* `Tools:` O agente pode utilizar várias ferramentas (`como pesquisa na web`, `calculadoras` ou `bancos de dados`) para coletar informações adicionais ou executar ações.\n",
    "\n",
    "* `Action:` Com base em seu raciocínio, o LLM decide uma ação a ser tomada em seu ambiente.\n",
    "\n",
    "* `Environment:` Representa o mundo externo ou contexto no qual o agente opera. `Pode ser um site`, `um espaço físico` ou `um ambiente simulado`.\n",
    "\n",
    "* `Result:` A ação tomada pelo agente produz um resultado no ambiente, que então realimenta a compreensão e as ações futuras do agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Exemplo de código</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Configurando Ollama e Dependências</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "ollama serve\n",
    "ollama pull llama3.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também:\n",
    "\n",
    "```\n",
    "pip install openai duckduckgo-search\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Criando um cliente OpenAI para interação LLM local</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "Eddy_key_openai  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=Eddy_key_openai,\n",
    "                base_url=\"http://127.0.0.1:11434/v1\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Implementando a funcionalidade de pesquisa</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso agente precisa da capacidade de pesquisar informações na `web`. Definimos uma função `search` que utiliza a biblioteca `duckduckgo-search` para consultar `DuckDuckGo` e recuperar resultados de pesquisa relevantes.\n",
    "\n",
    "\n",
    "\n",
    "`Esses resultados serão usados ​​para aumentar o conhecimento do LLM quando necessário.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from duckduckgo_search import DDGS\n",
    "# import logging\n",
    "# # Configurar logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# def search(query , max_results=3):\n",
    "#     \"\"\"\n",
    "#     Realiza uma pesquisa web usando DuckDuckGo e retorna os resultados.\n",
    "    \n",
    "#     Args:\n",
    "#         query (str): A consulta de pesquisa.\n",
    "#         max_results (int): O número máximo de resultados a retornar.\n",
    "    \n",
    "#     Returns:\n",
    "#         str: Uma string com os resultados da pesquisa.\n",
    "#     \"\"\"\n",
    "#     logging.info(f\"Realizando pesquisa na Internet para: {query}\")\n",
    "#     try:\n",
    "#         results = DDGS().text(query, max_results=max_results)\n",
    "#         logging.info(\"Pesquisa concluída.\")\n",
    "#         return str(\"\\n\".join(str(results[i]) for i in range(len(results))))\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Erro durante a pesquisa: {str(e)}\")\n",
    "#         return f\"Erro na pesquisa: {str(e)}\"\n",
    "\n",
    "\n",
    "\n",
    "# from duckduckgo_search import DDGS\n",
    "# import logging\n",
    "\n",
    "# # Configurar logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# def search(query, max_results=2):\n",
    "#     \"\"\"\n",
    "#     Realiza uma pesquisa web usando DuckDuckGo e retorna os resultados.\n",
    "    \n",
    "#     Args:\n",
    "#         query (str): A consulta de pesquisa.\n",
    "#         max_results (int): O número máximo de resultados a retornar.\n",
    "    \n",
    "#     Returns:\n",
    "#         str: Uma string com os resultados da pesquisa ou uma mensagem de erro.\n",
    "    \n",
    "#     Exemplos:\n",
    "#         >>> search(\"Python\")\n",
    "#         >>> search(\"DuckDuckGo\", max_results=5)\n",
    "#     \"\"\"\n",
    "#     logging.info(f\"Realizando pesquisa na Internet para: {query}\")\n",
    "#     try:\n",
    "#         results = DDGS().text(query, max_results=max_results)\n",
    "        \n",
    "#         if not results:\n",
    "#             logging.warning(\"Nenhum resultado encontrado.\")\n",
    "#             return \"Nenhum resultado encontrado.\"\n",
    "        \n",
    "#         logging.info(f\"Pesquisa concluída com {len(results)} resultados.\")\n",
    "#         return \"\\n\".join(str(result) for result in results)\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Erro durante a pesquisa: {str(e)}\")\n",
    "#         return f\"Erro na pesquisa: {str(e)}\"\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "from duckduckgo_search import DDGS\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def search(query, max_results=2):\n",
    "    \"\"\"\n",
    "    Realiza uma pesquisa web usando DuckDuckGo e retorna os resultados como texto.\n",
    "    \n",
    "    Args:\n",
    "        query (str): A consulta de pesquisa.\n",
    "        max_results (int): O número máximo de resultados a retornar.\n",
    "    \n",
    "    Returns:\n",
    "        str: Uma string com os resultados da pesquisa ou uma mensagem de erro.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Realizando pesquisa na Internet para: {query}\")\n",
    "    try:\n",
    "        results = DDGS().text(query, max_results=max_results)\n",
    "        \n",
    "        if not results:\n",
    "            logging.warning(\"Nenhum resultado encontrado.\")\n",
    "            return \"Nenhum resultado encontrado.\"\n",
    "        \n",
    "        logging.info(f\"Pesquisa concluída com {len(results)} resultados.\")\n",
    "        \n",
    "        # Extrair e formatar o conteúdo dos resultados\n",
    "        formatted_results = []\n",
    "        for result in results:\n",
    "            title = result.get('title', 'Título não disponível')\n",
    "            snippet = result.get('body', 'Conteúdo não disponível')\n",
    "            formatted_results.append(f\"{title}\\n{snippet}\\n\")\n",
    "\n",
    "        return \"\\n\".join(formatted_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro durante a pesquisa: {str(e)}\")\n",
    "        return f\"Erro na pesquisa: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Inicializa a pesquisa:\n",
    "with DDGS() as ddgs:\n",
    "    # Define a consulta de pesquisa:\n",
    "    query = \"Quem ganharam o prêmio Nobel de Física de 2024?\"\n",
    "    \n",
    "    # Realiza a pesquisa e limita os resultados a 2:\n",
    "    results = [r for r in ddgs.text(query, max_results=2)]\n",
    "    \n",
    "    # Exibe os resultados\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Definindo ação e extração de entrada</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para orientar o comportamento do agente, precisamos entender suas ações pretendidas com base na saída do `LLM`.\n",
    "\n",
    "\n",
    "Definimos uma função para extrair a `“Action”` e a `“Action Input”` da resposta do `LLM`, permitindo-nos determinar se ele pretende responder diretamente ao usuário ou realizar uma pesquisa na web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_action_and_input(text):\n",
    "  action_pattern = r\"Ação: (.+?)\\n\"\n",
    "  input_pattern = r\"Entrada de ação: \\\"(.+?)\\\"\"\n",
    "  action = re.findall(action_pattern, text)\n",
    "  action_input = re.findall(input_pattern, text)\n",
    "  logging.info(f\"Ação extraída: {action}, Input da ação: {action_input}\")\n",
    "  return action, action_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Elaborando o Prompt do Sistema</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O prompt do sistema (`system prompt`) fornece instruções ao `LLM` sobre como se comportar. Ele descreve as ferramentas disponíveis (`neste caso, pesquisa na web`) e o processo de tomada de decisão para escolher entre respostas diretas e utilizar a ferramenta de pesquisa.\n",
    "\n",
    "\n",
    "`Este prompt garante que o agente siga um fluxo de trabalho lógico.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "System_prompt = \"\"\"\n",
    "Responda às seguintes perguntas da melhor forma possível. Use a pesquisa na web somente se necessário. \n",
    "\n",
    "Você tem acesso à seguinte ferramenta: \n",
    "\n",
    "Pesquisa: útil para quando você precisa responder a perguntas sobre eventos atuais ou quando precisa de informações específicas que não conhece. \n",
    "          No entanto, você só deve usá-la se a consulta do usuário não puder ser respondida com conhecimento geral ou uma resposta direta. \n",
    "\n",
    "Você receberá uma mensagem do humano, então você deve iniciar um loop e fazer um dos seguintes: \n",
    "\n",
    "Opção 1: Responder ao humano diretamente \n",
    "         - Se a pergunta do usuário for simples, coloquial ou puder ser respondida com seu conhecimento existente, responda diretamente. \n",
    "         - Se o usuário não solicitou uma pesquisa ou a pergunta não precisa de informações atualizadas ou específicas, evite usar a ferramenta de pesquisa. \n",
    "\n",
    "Use o seguinte formato ao responder ao humano: \n",
    "Pensamento: Explique por que uma resposta direta é suficiente ou por que nenhuma pesquisa é necessária. \n",
    "Ação: Resposta ao Humano\n",
    "Entrada de ação: \"sua resposta ao humano, resumindo o que você sabe ou concluindo a conversa\" \n",
    "\n",
    "Opção 2: Use a ferramenta de pesquisa para responder à pergunta. \n",
    "         - Se precisar de mais informações para responder à pergunta, ou se a pergunta for sobre um evento atual ou conhecimento específico que pode não \n",
    "           estar nos seus dados de treinamento, use a ferramenta de pesquisa. \n",
    "         - Após receber os resultados da pesquisa, decida se você tem informações suficientes para responder à pergunta ou se outra pesquisa é necessária. \n",
    "\n",
    "Use o seguinte formato ao usar a ferramenta de pesquisa: \n",
    "Pensamento: Explique por que uma pesquisa é necessária ou por que pesquisas adicionais são necessárias. \n",
    "Ação: Pesquisar \n",
    "Entrada de ação: \"a entrada para a ação, a ser enviada para a ferramenta\" \n",
    "\n",
    "Lembre-se de sempre decidir cuidadosamente se deve pesquisar ou responder diretamente. Se puder responder à pergunta sem pesquisar, sempre prefira responder diretamente. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Construindo o Loop de Execução do Agente</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O núcleo (`core`) do nosso agente está na função `run_agent`. Esta função gerencia o loop de interação entre o `usuário`, o `LLM` e a `ferramenta de busca`.\n",
    "\n",
    "\n",
    "`Ele processa os prompts do usuário, os envia ao LLM, interpreta a resposta do LLM, realiza pesquisas quando necessário e, por fim, fornece uma resposta final ao usuário.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(prompt, system_prompt):\n",
    "    # Prepare the initial message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    previous_searches = set()  # Track previous search queries to avoid repetition\n",
    "\n",
    "    while True:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3.1:8b\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        response_text = response.choices[0].message.content\n",
    "        logging.info(f\"Resposta do modelo: {response_text}\")\n",
    "        print(response_text)\n",
    "\n",
    "        action, action_input = extract_action_and_input(response_text)\n",
    "        if not action or not action_input:\n",
    "            return \"Failed to parse action or action input.\"\n",
    "\n",
    "        if action[-1] == \"Search\":\n",
    "            search_query = action_input[-1].strip()\n",
    "\n",
    "            # Check if the search query has been performed before\n",
    "            if search_query in previous_searches:\n",
    "                print(\"Repeated search detected. Stopping to prevent infinite loop.\")\n",
    "                break\n",
    "\n",
    "            print(f\"================ Performing search for: {search_query} ================\")\n",
    "            observation = search(search_query)\n",
    "            print(\"================ Search completed !! ================\")\n",
    "            previous_searches.add(search_query)  # Add to the set of performed searches\n",
    "\n",
    "\n",
    "            messages.extend([\n",
    "            { \"role\": \"system\", \"content\": response_text },\n",
    "            { \"role\": \"user\", \"content\": f\"Observation: {observation}\" },\n",
    "            ])\n",
    "\n",
    "        elif action[-1] == \"Response To Human\":\n",
    "            print(f\"Response: {action_input[-1]}\")\n",
    "            break\n",
    "\n",
    "        # Prevent infinite looping\n",
    "        if len(previous_searches) > 3:\n",
    "            print(\"Too many searches performed. Ending to prevent infinite loop.\")\n",
    "            break\n",
    "\n",
    "    return action_input[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Exemplos ilustrativos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, demonstramos as capacidades do `agente` com dois exemplos.\n",
    "\n",
    "O primeiro mostra uma interação de conversação simples em que o agente responde diretamente a uma saudação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = run_agent(prompt=\"Olá!\",\n",
    "          system_prompt=System_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">O segundo exemplo demonstra um cenário em que o `agente` precisa realizar uma pesquisa na web para responder a uma pergunta sobre a estrutura `Ollama`, destacando sua capacidade de aproveitar `fontes externas de conhecimento`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = run_agent(prompt=\"Fale-me sobre o framework ollama\",\n",
    "          system_prompt=System_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = run_agent(prompt=\"Quem foram os ganhadores do Prêmio Nobel de Física de 2024?\",\n",
    "          system_prompt=System_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "Eddy_key_openai  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=Eddy_key_openai,\n",
    "                base_url=\"http://127.0.0.1:11434/v1\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "def search(query , max_results=10):\n",
    "  results = DDGS().text(query, max_results=max_results)\n",
    "  return str(\"\\n\".join(str(results[i]) for i in range(len(results))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_action_and_input(text):\n",
    "  action_pattern = r\"Action: (.+?)\\n\"\n",
    "  input_pattern = r\"Action Input: \\\"(.+?)\\\"\"\n",
    "  action = re.findall(action_pattern, text)\n",
    "  action_input = re.findall(input_pattern, text)\n",
    "  return action, action_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "System_prompt = \"\"\"\n",
    "Answer the following questions as best you can. Use web search only if necessary.\n",
    "\n",
    "You have access to the following tool:\n",
    "\n",
    "Search: useful for when you need to answer questions about current events or when you need specific information that you don't know. However, you should only use it if the user's query cannot be answered with general knowledge or a straightforward response.\n",
    "\n",
    "You will receive a message from the human, then you should start a loop and do one of the following:\n",
    "\n",
    "Option 1: Respond to the human directly\n",
    "- If the user's question is simple, conversational, or can be answered with your existing knowledge, respond directly.\n",
    "- If the user did not ask for a search or the question does not need updated or specific information, avoid using the search tool.\n",
    "\n",
    "Use the following format when responding to the human:\n",
    "Thought: Explain why a direct response is sufficient or why no search is needed.\n",
    "Action: Response To Human\n",
    "Action Input: \"your response to the human, summarizing what you know or concluding the conversation\"\n",
    "\n",
    "Option 2: Use the search tool to answer the question.\n",
    "- If you need more information to answer the question, or if the question is about a current event or specific knowledge that may not be in your training data, use the search tool.\n",
    "- After receiving search results, decide whether you have enough information to answer the question or if another search is necessary.\n",
    "\n",
    "Use the following format when using the search tool:\n",
    "Thought: Explain why a search is needed or why additional searches are needed.\n",
    "Action: Search\n",
    "Action Input: \"the input to the action, to be sent to the tool\"\n",
    "\n",
    "Remember to always decide carefully whether to search or respond directly. If you can answer the question without searching, always prefer responding directly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(prompt, system_prompt):\n",
    "    # Prepare the initial message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    previous_searches = set()  # Track previous search queries to avoid repetition\n",
    "\n",
    "    while True:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3.1:8b\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        response_text = response.choices[0].message.content\n",
    "        print(response_text)\n",
    "\n",
    "        action, action_input = extract_action_and_input(response_text)\n",
    "        if not action or not action_input:\n",
    "            return \"Failed to parse action or action input.\"\n",
    "\n",
    "        if action[-1] == \"Search\":\n",
    "            search_query = action_input[-1].strip()\n",
    "\n",
    "            # Check if the search query has been performed before\n",
    "            if search_query in previous_searches:\n",
    "                print(\"Repeated search detected. Stopping to prevent infinite loop.\")\n",
    "                break\n",
    "\n",
    "            print(f\"================ Performing search for: {search_query} ================\")\n",
    "            observation = search(search_query)\n",
    "            print(\"================ Search completed !! ================\")\n",
    "            previous_searches.add(search_query)  # Add to the set of performed searches\n",
    "\n",
    "\n",
    "            messages.extend([\n",
    "            { \"role\": \"system\", \"content\": response_text },\n",
    "            { \"role\": \"user\", \"content\": f\"Observation: {observation}\" },\n",
    "            ])\n",
    "\n",
    "        elif action[-1] == \"Response To Human\":\n",
    "            print(f\"Response: {action_input[-1]}\")\n",
    "            break\n",
    "\n",
    "        # Prevent infinite looping\n",
    "        if len(previous_searches) > 3:\n",
    "            print(\"Too many searches performed. Ending to prevent infinite loop.\")\n",
    "            break\n",
    "\n",
    "    return action_input[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: A search is needed because the question refers to a future specific knowledge that may not be in my training data.\n",
      "\n",
      "Action: Search\n",
      "Action Input: \"the 2024 Nobel Prize winners in physics\"\n",
      "================ Performing search for: the 2024 Nobel Prize winners in physics ================\n",
      "================ Search completed !! ================\n",
      "Thought: I have found the necessary information using the search tool.\n",
      "\n",
      "Action: Respond\n",
      "Action Input: \"John J. Hopfield from Princeton University and Geoffrey E. Hinton from the University of Toronto were awarded the 2024 Nobel Prize in Physics, 'for foundational discoveries and inventions that enable machine learning with artificial neural networks.'\"\n",
      "Thought: The information about the winners of the 2024 Nobel Prize in Physics is readily available from multiple sources, including news articles and official Nobel Prize websites.\n",
      "\n",
      "Action: Response To Human\n",
      "Action Input: \"The winners of the 2024 Nobel Prize in Physics are John J. Hopfield of Princeton University and Geoffrey E. Hinton of the University of Toronto, who were awarded the prize for their foundational discoveries and inventions that enable machine learning with artificial neural networks.\"\n",
      "Response: The winners of the 2024 Nobel Prize in Physics are John J. Hopfield of Princeton University and Geoffrey E. Hinton of the University of Toronto, who were awarded the prize for their foundational discoveries and inventions that enable machine learning with artificial neural networks.\n"
     ]
    }
   ],
   "source": [
    "response = run_agent(prompt=\"Who were the winners of the 2024 Nobel Prize in Physics?\",\n",
    "          system_prompt=System_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_All",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
