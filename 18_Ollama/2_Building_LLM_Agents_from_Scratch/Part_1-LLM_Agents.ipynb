{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"><font color=\"red\">How to Build a Simple Reasoning and Acting Agent from Scratch</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este Notebook est√° baseado no Tutorial de [kirouane Ayoub]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links de estudo:\n",
    "\n",
    "* [Part 1](https://blog.gopenai.com/introduction-to-llm-agents-how-to-build-a-simple-reasoning-and-acting-agent-from-scratch-part-1-843e14686be7)\n",
    "\n",
    "* [Part 2](https://blog.gopenai.com/building-llm-agents-from-scratch-part-2-a-conversational-search-agent-with-ollama-a4544b2291cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Contextualizando</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta Notebook inicia uma s√©rie sobre a constru√ß√£o de `agentes de IA¬¥, come√ßando com uma implementa√ß√£o b√°sica usando `Ollama¬¥ para executar `LLMs¬¥ localmente. O objetivo √© criar um sistema que compreenda consultas do usu√°rio e utilize pesquisa na web para fornecer respostas informativas, demonstrando os conceitos fundamentais do design de agentes de IA.\n",
    "\n",
    "Esta implementa√ß√£o serve como um ponto de partida para entender o desenvolvimento de `agentes de IA`, mas √© uma vers√£o simplificada. Em cen√°rios reais, os agentes teriam comportamentos mais complexos, exigindo t√©cnicas de an√°lise sint√°tica robustas para lidar com sa√≠das inesperadas. Al√©m disso, seriam necess√°rias estrat√©gias abrangentes de tratamento de erros para garantir uma opera√ß√£o suave e confi√°vel do sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Funcionamento do Agent</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*-c8XfC8zGlrmF8DMwWkJaA.png\" alt=\"Minha Imagem\" width=\"700\" height=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Agent:` Representa o sistema geral que abrange o LLM e suas intera√ß√µes com o ambiente.\n",
    "\n",
    "* `Task:` Este √© o objetivo ou meta que o agente est√° tentando atingir. √â o ponto de partida do processo.\n",
    "\n",
    "* `LLM:` Este √© o n√∫cleo do agente, um LLM que processa informa√ß√µes e toma decis√µes.\n",
    "\n",
    "* `Reasoning:` O LLM utiliza o racioc√≠nio para determinar o melhor curso de a√ß√£o com base na tarefa e nas informa√ß√µes dispon√≠veis.\n",
    "\n",
    "* `Tools:` O agente pode utilizar v√°rias ferramentas (`como pesquisa na web`, `calculadoras` ou `bancos de dados`) para coletar informa√ß√µes adicionais ou executar a√ß√µes.\n",
    "\n",
    "* `Action:` Com base em seu racioc√≠nio, o LLM decide uma a√ß√£o a ser tomada em seu ambiente.\n",
    "\n",
    "* `Environment:` Representa o mundo externo ou contexto no qual o agente opera. `Pode ser um site`, `um espa√ßo f√≠sico` ou `um ambiente simulado`.\n",
    "\n",
    "* `Result:` A a√ß√£o tomada pelo agente produz um resultado no ambiente, que ent√£o realimenta a compreens√£o e as a√ß√µes futuras do agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"gree\">Exemplo de c√≥digo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Configurando Ollama e Depend√™ncias</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "ollama serve\n",
    "ollama pull llama3.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tamb√©m:\n",
    "\n",
    "```\n",
    "pip install openai duckduckgo-search\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Criando um cliente OpenAI para intera√ß√£o LLM local</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "Eddy_key_openai  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=Eddy_key_openai,\n",
    "                base_url=\"http://127.0.0.1:11434/v1\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Implementando a funcionalidade de pesquisa</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso agente precisa da capacidade de pesquisar informa√ß√µes na `web`. Definimos uma fun√ß√£o `search` que utiliza a biblioteca `duckduckgo-search` para consultar `DuckDuckGo` e recuperar resultados de pesquisa relevantes.\n",
    "\n",
    "\n",
    "\n",
    "`Esses resultados ser√£o usados ‚Äã‚Äãpara aumentar o conhecimento do LLM quando necess√°rio.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from duckduckgo_search import DDGS\n",
    "\n",
    "# def search(query , max_results=10):\n",
    "#   results = DDGS().text(query, max_results=max_results)\n",
    "#   return str(\"\\n\".join(str(results[i]) for i in range(len(results))))\n",
    "\n",
    "\n",
    "from duckduckgo_search import DDGS\n",
    "import logging\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def search(query , max_results=3):\n",
    "    \"\"\"\n",
    "    Realiza uma pesquisa web usando DuckDuckGo e retorna os resultados.\n",
    "    \n",
    "    Args:\n",
    "        query (str): A consulta de pesquisa.\n",
    "        max_results (int): O n√∫mero m√°ximo de resultados a retornar.\n",
    "    \n",
    "    Returns:\n",
    "        str: Uma string com os resultados da pesquisa.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Realizando pesquisa na Internet para: {query}\")\n",
    "    try:\n",
    "        results = DDGS().text(query, max_results=max_results)\n",
    "        logging.info(\"Research completed.\")\n",
    "        return str(\"\\n\".join(str(results[i]) for i in range(len(results))))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during search: {str(e)}\")\n",
    "        return f\"Error during search: {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Press release: The Nobel Prize in Physics 2024 - NobelPrize.org', 'href': 'https://www.nobelprize.org/prizes/physics/2024/press-release/', 'body': '8 October 2024. The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Physics 2024 to. John J. Hopfield. Princeton University, NJ, USA. Geoffrey E. Hinton. University of Toronto, Canada. \"for foundational discoveries and inventions that enable machine learning with artificial neural networks\".'}\n",
      "{'title': 'Nobel Prize in physics 2024 awarded for work on artificial intelligence ...', 'href': 'https://www.cnn.com/2024/10/08/science/nobel-prize-physics-hopfield-hinton-machine-learning-intl/index.html', 'body': 'The 2024 Nobel Prize in physics has been awarded to John Hopfield and Geoffrey Hinton for their fundamental discoveries in machine learning, which paved the way for how artificial intelligence is ...'}\n"
     ]
    }
   ],
   "source": [
    "# Testando o Duck:\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Inicializa a pesquisa:\n",
    "with DDGS() as ddgs:\n",
    "    # Define a consulta de pesquisa:\n",
    "    query = \"Who won the 2024 Nobel Prize in Physics?\"\n",
    "    \n",
    "    # Realiza a pesquisa e limita os resultados a 2:\n",
    "    results = [r for r in ddgs.text(query, max_results=2)]\n",
    "    \n",
    "    # Exibe os resultados\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Definindo a√ß√£o e extra√ß√£o de entrada</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para orientar o comportamento do agente, precisamos entender suas a√ß√µes pretendidas com base na sa√≠da do `LLM`.\n",
    "\n",
    "\n",
    "Definimos uma fun√ß√£o para extrair a `‚ÄúAction‚Äù` e a `‚ÄúAction Input‚Äù` da resposta do `LLM`, permitindo-nos determinar se ele pretende responder diretamente ao usu√°rio ou realizar uma pesquisa na web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_action_and_input(text):\n",
    "  action_pattern = r\"Action: (.+?)\\n\"\n",
    "  input_pattern = r\"Action Input: \\\"(.+?)\\\"\"\n",
    "  action = re.findall(action_pattern, text)\n",
    "  action_input = re.findall(input_pattern, text)\n",
    "  logging.info(f\"Action extracted: {action}, Action input: {action_input}\")\n",
    "  return action, action_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Elaborando o Prompt do Sistema</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O prompt do sistema (`system prompt`) fornece instru√ß√µes ao `LLM` sobre como se comportar. Ele descreve as ferramentas dispon√≠veis (`neste caso, pesquisa na web`) e o processo de tomada de decis√£o para escolher entre respostas diretas e utilizar a ferramenta de pesquisa.\n",
    "\n",
    "\n",
    "`Este prompt garante que o agente siga um fluxo de trabalho l√≥gico.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "System_prompt = \"\"\"\n",
    "Answer the following questions as best you can. Use web search only if necessary.\n",
    "\n",
    "You have access to the following tool:\n",
    "\n",
    "Search: useful for when you need to answer questions about current events or when you need specific information that you don't know. However, you should only use it if the user's query cannot be answered with general knowledge or a straightforward response.\n",
    "\n",
    "You will receive a message from the human, then you should start a loop and do one of the following:\n",
    "\n",
    "Option 1: Respond to the human directly\n",
    "- If the user's question is simple, conversational, or can be answered with your existing knowledge, respond directly.\n",
    "- If the user did not ask for a search or the question does not need updated or specific information, avoid using the search tool.\n",
    "\n",
    "Use the following format when responding to the human:\n",
    "Thought: Explain why a direct response is sufficient or why no search is needed.\n",
    "Action: Response To Human\n",
    "Action Input: \"your response to the human, summarizing what you know or concluding the conversation\"\n",
    "\n",
    "Option 2: Use the search tool to answer the question.\n",
    "- If you need more information to answer the question, or if the question is about a current event or specific knowledge that may not be in your training data, use the search tool.\n",
    "- After receiving search results, decide whether you have enough information to answer the question or if another search is necessary.\n",
    "\n",
    "Use the following format when using the search tool:\n",
    "Thought: Explain why a search is needed or why additional searches are needed.\n",
    "Action: Search\n",
    "Action Input: \"the input to the action, to be sent to the tool\"\n",
    "\n",
    "Remember to always decide carefully whether to search or respond directly. If you can answer the question without searching, always prefer responding directly.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Construindo o Loop de Execu√ß√£o do Agente</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O n√∫cleo (`core`) do nosso agente est√° na fun√ß√£o `run_agent`. Esta fun√ß√£o gerencia o loop de intera√ß√£o entre o `usu√°rio`, o `LLM` e a `ferramenta de busca`.\n",
    "\n",
    "\n",
    "`Ele processa os prompts do usu√°rio, os envia ao LLM, interpreta a resposta do LLM, realiza pesquisas quando necess√°rio e, por fim, fornece uma resposta final ao usu√°rio.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(prompt, system_prompt):\n",
    "    # Prepare the initial message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    previous_searches = set()  # Track previous search queries to avoid repetition\n",
    "\n",
    "    while True:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3.1:8b\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        response_text = response.choices[0].message.content\n",
    "        #print(\"ü§ó\", response_text, \"ü§ó\")\n",
    "\n",
    "        action, action_input = extract_action_and_input(response_text)\n",
    "        if not action or not action_input:\n",
    "            return \"Failed to parse action or action input.\"\n",
    "\n",
    "        if action[-1] == \"Search\":\n",
    "            search_query = action_input[-1].strip()\n",
    "\n",
    "            # Verifique se a consulta de pesquisa j√° foi realizada antes:\n",
    "            if search_query in previous_searches:\n",
    "                print(\"Repeated search detected. Stopping to prevent infinite loop.\")\n",
    "                break\n",
    "\n",
    "            print(f\"================ Performing search for: {search_query} ================\")\n",
    "            observation = search(search_query)\n",
    "            print(\"================ Search completed !! ================\")\n",
    "            previous_searches.add(search_query)  # Adicionar ao conjunto de pesquisas realizadas\n",
    "\n",
    "\n",
    "            messages.extend([\n",
    "            { \"role\": \"system\", \"content\": response_text },\n",
    "            { \"role\": \"user\", \"content\": f\"Observation: {observation}\" },\n",
    "            ])\n",
    "\n",
    "        elif action[-1] == \"Response To Human\":\n",
    "            print(f\"Response: {action_input[-1]}\")\n",
    "            break\n",
    "\n",
    "        # Evitar loop infinito:\n",
    "        if len(previous_searches) > 3:\n",
    "            print(\"Too many searches performed. Ending to prevent infinite loop.\")\n",
    "            break\n",
    "\n",
    "    return action_input[-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"yellow\">Exemplos ilustrativos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, demonstramos as capacidades do `agente` com dois exemplos.\n",
    "\n",
    "O primeiro mostra uma intera√ß√£o de conversa√ß√£o simples em que o agente responde diretamente a uma sauda√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:10,946 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:10,947 - INFO - Action extracted: ['Response To Human'], Action input: ['Hello! Nice to meet you!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Hello! Nice to meet you!\n"
     ]
    }
   ],
   "source": [
    "response = run_agent(prompt=\"Hello!!\",\n",
    "          system_prompt=System_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Nice to meet you!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">O segundo exemplo demonstra um cen√°rio em que o `agente` precisa realizar uma pesquisa na web para responder a uma pergunta sobre a estrutura `Ollama`, destacando sua capacidade de aproveitar `fontes externas de conhecimento`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:28,381 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:28,382 - INFO - Action extracted: ['Search'], Action input: ['ollama framework']\n",
      "2024-10-13 16:02:28,382 - INFO - Realizando pesquisa na Internet para: ollama framework\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Performing search for: ollama framework ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:29,402 - INFO - Research completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Search completed !! ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:33,327 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:33,328 - INFO - Action extracted: ['Response To Human'], Action input: ['The ollama framework is a lightweight, extensible framework for building and running language models on a local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in various applications.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The ollama framework is a lightweight, extensible framework for building and running language models on a local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in various applications.\n"
     ]
    }
   ],
   "source": [
    "response = run_agent(prompt=\"Tell me about the ollama framework\",\n",
    "          system_prompt=System_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ollama framework is a lightweight, extensible framework for building and running language models on a local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in various applications.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:15,759 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:15,760 - INFO - Action extracted: ['Search'], Action input: ['the winners of the 2024 Nobel Prize in Physics']\n",
      "2024-10-13 16:02:15,760 - INFO - Realizando pesquisa na Internet para: the winners of the 2024 Nobel Prize in Physics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Performing search for: the winners of the 2024 Nobel Prize in Physics ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:17,133 - INFO - Research completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Search completed !! ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 16:02:19,777 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:19,778 - INFO - Action extracted: ['Respond To Human'], Action input: ['The winners of the 2024 Nobel Prize in Physics were John J. Hopfield and Geoffrey E. Hinton.']\n",
      "2024-10-13 16:02:22,381 - INFO - HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-13 16:02:22,382 - INFO - Action extracted: ['Response To Human'], Action input: ['The winners of the 2024 Nobel Prize in Physics were John J. Hopfield from Princeton University, USA, and Geoffrey E. Hinton from the University of Toronto, Canada. They were awarded for their foundational discoveries and inventions that enabled machine learning with artificial neural networks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The winners of the 2024 Nobel Prize in Physics were John J. Hopfield from Princeton University, USA, and Geoffrey E. Hinton from the University of Toronto, Canada. They were awarded for their foundational discoveries and inventions that enabled machine learning with artificial neural networks.\n"
     ]
    }
   ],
   "source": [
    "response = run_agent(prompt=\"Who were the winners of the 2024 Nobel Prize in Physics?\",\n",
    "          system_prompt=System_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winners of the 2024 Nobel Prize in Physics were John J. Hopfield from Princeton University, USA, and Geoffrey E. Hinton from the University of Toronto, Canada. They were awarded for their foundational discoveries and inventions that enabled machine learning with artificial neural networks.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_All",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
