{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"><font color=\"gree\">Pydantic.AI</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook foi baseado no tutorial de [Plaban Nayak](https://medium.com/the-ai-forum/building-an-agentic-system-to-enhance-rag-with-self-grading-and-web-search-capabilities-using-3f9a1d885730)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*I-kwOIjL3WrD3NL0WKLcYQ.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, ModelRetry\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.models.groq import GroqModel\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "Eddy_key_openai  = os.environ['OPENAI_API_KEY']\n",
    "Eddy_key_groq = os.environ['GROQ_API_KEY']\n",
    "Eddy_key_gemini = os.environ['GEMINI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = OpenAIModel('gpt-4o-mini')\n",
    "groq_model = GroqModel(\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    'gemini-1.5-flash',\n",
    "    system_prompt='Seja muito conciso, responda com uma frase apenas.',\n",
    "    retries=3\n",
    ")\n",
    "\n",
    "result = agent.run_sync('Onde \"hello world\" vem?')\n",
    "\n",
    "result.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model = 'gemini-1.5-pro'\n",
    "\n",
    "@agent.system_prompt\n",
    "async def get_system_prompt(self) -> str:\n",
    "    return \"Gere uma resposta longa e densa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "result = agent.run_sync('Onde \"hello world\" vem?')\n",
    "Markdown(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Basic structured output</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "    reason: str\n",
    "    famous_person_from_city: str\n",
    "\n",
    "\n",
    "model = 'openai:gpt-4o'\n",
    "\n",
    "agent = Agent(model,\n",
    "              result_type=MyModel,\n",
    "              )\n",
    "\n",
    "\n",
    "result = agent.run_sync('A cidade ventosa nos EUA.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.run_sync('A cidade do Merlion.')\n",
    "\n",
    "result.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UM exemplo de meu amado Perú:\n",
    "result = agent.run_sync('A cidade onde está o Machu Picchu')\n",
    "\n",
    "result.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.data.famous_person_from_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Chat APP</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "agent = Agent('openai:gpt-4o-mini',\n",
    "              system_prompt=\"\"\"Seja um assistente útil e amigável.\n",
    "                            \"\"\",\n",
    "              retries=2,\n",
    "              result_type=str\n",
    "             )\n",
    "\n",
    "result = agent.run_sync('Me conte uma piada.')\n",
    "\n",
    "result.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todos os messages da execução:\n",
    "pprint(result.all_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Chatting LLMs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(model='openai:gpt-4o-mini',\n",
    "              system_prompt='Seja um assistente útil e amigável.',\n",
    "              retries=3,\n",
    "              result_type=str\n",
    "              )\n",
    "\n",
    "result1 = agent.run_sync('Me conte uma piada.')\n",
    "result1.data\n",
    "\n",
    "result2 = agent.run_sync('Explique a piada.',\n",
    "                        model='gemini-1.5-pro',\n",
    "                        message_history=result1.new_messages()\n",
    "                        )\n",
    "result2.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Weather</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations as _annotations\n",
    "import asyncio\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from devtools import debug\n",
    "from httpx import AsyncClient\n",
    "from pydantic_ai import Agent, ModelRetry, RunContext\n",
    "import logfire\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Deps:\n",
    "    client: AsyncClient\n",
    "    weather_api_key: str | None\n",
    "    geo_api_key: str | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_agent = Agent('openai:gpt-4o',\n",
    "                      system_prompt='Seja conciso, responda com uma frase apenas.',\n",
    "                      deps_type=Deps,\n",
    "                      retries=2\n",
    "                     )\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_lat_lng(\n",
    "    ctx: RunContext[Deps], location_description: str\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Encontre a latitude e longitude de uma localização.\n",
    "\n",
    "    Args:\n",
    "        ctx: O contexto.\n",
    "        location_description: A descrição de uma localização.\n",
    "    \"\"\"\n",
    "    if ctx.deps.geo_api_key is None:\n",
    "        # Se não fornecido uma API key, retorne uma resposta fictícia (Londres):\n",
    "        return {'lat': 51.1, 'lng': -0.1}\n",
    "\n",
    "    params = {\n",
    "        'q': location_description,\n",
    "        'api_key': ctx.deps.geo_api_key,\n",
    "    }\n",
    "    with logfire.span('chamando API de geocodificação', params=params) as span:\n",
    "        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        span.set_attribute('Resposta', data)\n",
    "\n",
    "    if data:\n",
    "        return {'lat': data[0]['lat'], 'lng': data[0]['lon']}\n",
    "    else:\n",
    "        raise ModelRetry('Não foi possível encontrar a localização')\n",
    "\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n",
    "    \"\"\"Obtenha o clima em uma localização.\n",
    "\n",
    "    Args:\n",
    "        ctx: O contexto.\n",
    "        lat: Latitude da localização.\n",
    "        lng: Longitude da localização.\n",
    "    \"\"\"\n",
    "    if ctx.deps.weather_api_key is None:\n",
    "        # Se não fornecido uma API key, retorne uma resposta fictícia\n",
    "        return {'temperature': '21 °C', 'description': 'Sunny'}\n",
    "\n",
    "    params = {\n",
    "        'apikey': ctx.deps.weather_api_key,\n",
    "        'location': f'{lat},{lng}',\n",
    "        'units': 'metric',\n",
    "    }\n",
    "    with logfire.span('chamando API de clima', params=params) as span:\n",
    "        r = await ctx.deps.client.get(\n",
    "            'https://api.tomorrow.io/v4/weather/realtime', params=params\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        span.set_attribute('Resposta', data)\n",
    "\n",
    "    values = data['data']['values']\n",
    "    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n",
    "    code_lookup = {\n",
    "        1000: 'Clear, Sunny',\n",
    "        1100: 'Mostly Clear',\n",
    "        1101: 'Partly Cloudy',\n",
    "        1102: 'Mostly Cloudy',\n",
    "        1001: 'Cloudy',\n",
    "        2000: 'Fog',\n",
    "        2100: 'Light Fog',\n",
    "        4000: 'Drizzle',\n",
    "        4001: 'Rain',\n",
    "        4200: 'Light Rain',\n",
    "        4201: 'Heavy Rain',\n",
    "        5000: 'Snow',\n",
    "        5001: 'Flurries',\n",
    "        5100: 'Light Snow',\n",
    "        5101: 'Heavy Snow',\n",
    "        6000: 'Freezing Drizzle',\n",
    "        6001: 'Freezing Rain',\n",
    "        6200: 'Light Freezing Rain',\n",
    "        6201: 'Heavy Freezing Rain',\n",
    "        7000: 'Ice Pellets',\n",
    "        7101: 'Heavy Ice Pellets',\n",
    "        7102: 'Light Ice Pellets',\n",
    "        8000: 'Thunderstorm',\n",
    "    }\n",
    "    return {\n",
    "        'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n",
    "        'description': code_lookup.get(values['weatherCode'], 'Unknown'),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    async with AsyncClient() as client:\n",
    "        # Crie uma chave API gratuita em https://www.tomorrow.io/weather-api/\n",
    "        weather_api_key = os.getenv('WEATHER_API_KEY')\n",
    "        # Crie uma chave API gratuita em https://geocode.maps.co/\n",
    "        geo_api_key = os.getenv('GEO_API_KEY')\n",
    "        deps = Deps(\n",
    "            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n",
    "        )\n",
    "        result = await weather_agent.run(\n",
    "            'Qual é o clima em Londres e em Singapura?', deps=deps\n",
    "        )\n",
    "        debug(result)\n",
    "        print('Resposta:', result.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Streaming MarkDown</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[23:31:45] </span><span style=\"color: #008080; text-decoration-color: #008080\">Perguntando: Mostre-me um exemplo curto de como usar Pydantic</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span><span style=\"color: #008080; text-decoration-color: #008080\">.                      </span> <a href=\"file:///tmp/ipykernel_101622/3873954971.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3873954971.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_101622/3873954971.py#30\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[23:31:45]\u001b[0m\u001b[2;36m \u001b[0m\u001b[36mPerguntando: Mostre-me um exemplo curto de como usar Pydantic\u001b[0m\u001b[33m...\u001b[0m\u001b[36m.                      \u001b[0m \u001b]8;id=450562;file:///tmp/ipykernel_101622/3873954971.py\u001b\\\u001b[2m3873954971.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=711774;file:///tmp/ipykernel_101622/3873954971.py#30\u001b\\\u001b[2m30\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Usando modelo: gemini-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5</span>-flash                                                         <a href=\"file:///tmp/ipykernel_101622/3873954971.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3873954971.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_101622/3873954971.py#33\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mUsando modelo: gemini-\u001b[1;36m1.5\u001b[0m-flash                                                         \u001b]8;id=589669;file:///tmp/ipykernel_101622/3873954971.py\u001b\\\u001b[2m3873954971.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=309366;file:///tmp/ipykernel_101622/3873954971.py#33\u001b\\\u001b[2m33\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cc47ffc60f4cd0a529b17b49637293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[23:31:49] </span>Usando modelo: openai:gpt-4o-mini                                                       <a href=\"file:///tmp/ipykernel_101622/3873954971.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3873954971.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_101622/3873954971.py#33\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[23:31:49]\u001b[0m\u001b[2;36m \u001b[0mUsando modelo: openai:gpt-4o-mini                                                       \u001b]8;id=868991;file:///tmp/ipykernel_101622/3873954971.py\u001b\\\u001b[2m3873954971.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=384121;file:///tmp/ipykernel_101622/3873954971.py#33\u001b\\\u001b[2m33\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72bf3a6569440f295f433c2b3af13a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[23:31:55] </span>Usando modelo: groq:llama3-70b-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>                                                     <a href=\"file:///tmp/ipykernel_101622/3873954971.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3873954971.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_101622/3873954971.py#33\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[23:31:55]\u001b[0m\u001b[2;36m \u001b[0mUsando modelo: groq:llama3-70b-\u001b[1;36m8192\u001b[0m                                                     \u001b]8;id=787783;file:///tmp/ipykernel_101622/3873954971.py\u001b\\\u001b[2m3873954971.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=794888;file:///tmp/ipykernel_101622/3873954971.py#33\u001b\\\u001b[2m33\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e88ab01f7e4417bdc1382674f95187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "\n",
    "from rich.console import Console, ConsoleOptions, RenderResult\n",
    "from rich.live import Live\n",
    "from rich.markdown import CodeBlock, Markdown\n",
    "from rich.syntax import Syntax\n",
    "from rich.text import Text\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models import KnownModelName\n",
    "\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "# Modelos para tentar, e a variável de ambiente apropriada:\n",
    "models: list[tuple[KnownModelName, str]] = [\n",
    "    ('gemini-1.5-flash', 'GEMINI_API_KEY'),\n",
    "    ('openai:gpt-4o-mini', 'OPENAI_API_KEY'),\n",
    "    ('groq:llama3-70b-8192', 'GROQ_API_KEY'),\n",
    "]\n",
    "\n",
    "\n",
    "async def main():\n",
    "    prettier_code_blocks()\n",
    "    console = Console()\n",
    "    prompt = 'Mostre-me um exemplo curto de como usar Pydantic.'\n",
    "    console.log(f'Perguntando: {prompt}...', style='cyan')\n",
    "    for model, env_var in models:\n",
    "        if env_var in os.environ:\n",
    "            console.log(f'Usando modelo: {model}')\n",
    "            with Live('', console=console, vertical_overflow='visible') as live:\n",
    "                async with agent.run_stream(prompt, model=model) as result:\n",
    "                    async for message in result.stream():\n",
    "                        live.update(Markdown(message))\n",
    "            #console.log(result.cost())\n",
    "        else:\n",
    "            console.log(f'{model} requer que {env_var} seja definido.')\n",
    "\n",
    "\n",
    "def prettier_code_blocks():\n",
    "    \"\"\"Faça que os blocos de código do rich sejam mais bonitos e fáceis de copiar.\n",
    "\n",
    "    De https://github.com/samuelcolvin/aicli/blob/v0.8.0/samuelcolvin_aicli.py#L22\n",
    "    \"\"\"\n",
    "\n",
    "    class SimpleCodeBlock(CodeBlock):\n",
    "        def __rich_console__(\n",
    "            self, console: Console, options: ConsoleOptions\n",
    "        ) -> RenderResult:\n",
    "            code = str(self.text).rstrip()\n",
    "            yield Text(self.lexer_name, style='dim')\n",
    "            yield Syntax(\n",
    "                code,\n",
    "                self.lexer_name,\n",
    "                theme=self.theme,\n",
    "                background_color='black',\n",
    "                word_wrap=True,\n",
    "            )\n",
    "            yield Text(f'/{self.lexer_name}', style='dim')\n",
    "\n",
    "    Markdown.elements['fence'] = SimpleCodeBlock\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
