{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"><font color=\"gree\">Qwen2-VL: Hands-On Guides for Invoice Data Extraction, Video Chatting, and Multimodal RAG with PDFs</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este Notebook segue o tutorial de [Nikos Kafritsas](https://aihorizonforecast.substack.com/p/qwen2-vl-hands-on-guides-for-invoice). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Como usar um modelo de linguagem de visão de código aberto de ponta?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A imagem abaixo mostra a `arquitetura` de nível superior do `Qwen2-VL`</font>\n",
    "\n",
    "\n",
    "O que se sabe até agora é que o `Qwen2-VL` usa o `Qwen2-LM` com um `Vision Transformer` — capaz de processar `imagens` e `vídeos`.\n",
    "\n",
    "Além disso, o `Qwen2-VL` introduz o novo `Multimodal Rotary Position Embeddings` (**M-ROPE**). Essa é uma variante dos `embeddings ROPE` (nós os discutimos brevemente [aqui](https://aihorizonforecast.substack.com/p/moirai-salesforces-foundation-transformer)) que decompõe os embeddings posicionais em partes — capturando `texto 1D`, `visual 2D` e `vídeo 3D`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf04e083-f0ef-4a9b-8784-ebecbfc52a78_1634x412.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Qwen2-VL` suporta vários idiomas, incluindo a maioria dos `idiomas europeus`, `japonês`, `coreano`, `chinês` e `árabe`. Mais detalhes estarão disponíveis quando o artigo oficial for lançado.\n",
    "\n",
    "\n",
    "<font color=\"red\">Nota:</font>\n",
    "\n",
    "`Qwen2-VL` vem em 3 tamanhos: `Qwen2-VL-2B`, `Qwen2-VL-7B` e `Qwen2-VL-72B`. As versões `2B` e `7B` são totalmente de `código aberto` (`Licença Apache`). A versão `72B é comercial`, mas gratuita para serviços com menos de 100 milhões de usuários ativos mensais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe3c34e0-f1e9-4fc8-9c73-915794e384e5_1656x1076.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O aumento de novos `LLMs` e `vLLMs` é imparável. Um modelo notável é o `Qwen2-VL` — um `Vision LM` com compreensão avançada de `imagem` e `vídeo`.\n",
    "\n",
    "Principais características do `Qwen2-VL`:\n",
    "\n",
    "* `Arquitetura poderosa:` Lançado pela `Alibaba`, é construído no Qwen2 LM.\n",
    "\n",
    "* `Reconhecimento de objetos e textos:` O `Qwen2-VL` reconhece relacionamentos complexos de objetos, textos manuscritos e vários idiomas.\n",
    "\n",
    "* `Raciocínio visual aprimorado:` o modelo resolve problemas de `matemática` e `codificação` a partir de gráficos e imagens, melhorando a extração de dados do mundo real.\n",
    "\n",
    "* `Compreensão de vídeo e bate-papo ao vivo:` fornece resumo de vídeo, resposta a perguntas e suporte por bate-papo em tempo real a partir de conteúdo de vídeo.\n",
    "\n",
    "* `Chamada de função (Function calling) e interações visuais:` o `Qwen2-VL` recupera dados em tempo real por meio de indicações visuais, como atualizações meteorológicas.\n",
    "\n",
    "\n",
    "\n",
    "Aqui seguimos o tutorial de `Nikos Kafritsas`, quem apresenta `3` guias práticos para usar o `Qwen2-VL`. Esses são:\n",
    "\n",
    "`1.` Extração de dados de imagem\n",
    "\n",
    "`2.` Conversando com vídeos\n",
    "\n",
    "`3.` RAG multimodal com PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">`Project 1:` Invoice Data Extraction to JSON Format</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste miniprojeto, extrairemos informações financeiras e pessoais de uma fatura — no formato `JSON`.\n",
    "\n",
    "Bibliotecas necessárias:\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/huggingface/transformers accelerate\n",
    "\n",
    "pip install qwen-vl-utils\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, baixamos nosso arquivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# We will use this sample invoice:\n",
    "url = \"https://www.zoho.com/invoice/images/invoice-templates/service-invoice-template/service-invoice-template-1x.jpg\"\n",
    "\n",
    "# Download the file:\n",
    "file_name = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, file_name)\n",
    "\n",
    "print(f\"Downloaded file name: {file_name}\")\n",
    "# Downloaded file name: service-invoice-template-1x.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, instalaremos `Qwen2-VL-2B-Instruct` — a versão menor, menos precisa, mas compatível com `VRAM` para execução no Colab.\n",
    "\n",
    "Sinta-se à vontade para tentar `Qwen2-VL-7B-Instruct` se você tiver uma `GPU` maior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import json\n",
    "\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o modelo ser baixado e colocado na memória, podemos enviar nossa solicitação. Algumas dicas extras:\n",
    "\n",
    "Use pelo menos as dimensões originais da imagem : certifique-se de usar pelo menos o tamanho original da sua imagem para obter melhores resultados ( resized_height e resized_width arguments abaixo)\n",
    "\n",
    "Dimensões maiores : Tamanhos de dimensões ligeiramente maiores em imagens de baixa qualidade podem melhorar a precisão, mas aumentarão o uso de VRAM. Ajuste de acordo:\n",
    "\n",
    "Usaremos o modelo de bate-papo do Qwen2-VL com o seguinte prompt:\n",
    "\n",
    "```\n",
    "\"Retrieve invoice_number, date_of_issue, seller_info, client_info, invoice_items_table, invoice_summary_table. Response must be in JSON format\"\n",
    "```\n",
    "ou em português:\n",
    "\n",
    "```\n",
    "Recuperar invoice_number, date_of_issue, seller_info, client_info, invoice_items_table, invoice_summary_table. A resposta deve estar no formato JSON\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": file_name,\n",
    "                \"resized_height\": 696,\n",
    "                \"resized_width\": 943,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\"Recuperar invoice_number, date_of_issue, seller_info, client_info,\n",
    "                           invoice_items_table, invoice_summary_table. A resposta deve estar\n",
    "                           no formato JSON\"\"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Você pode usar o código abaixo para corrigir possíveis erros e formatar a saída `JSON` do modelo:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = output_text[0]\n",
    "\n",
    "json_string = json_string.strip(\"[]'\")\n",
    "json_string = json_string.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\")\n",
    "json_string = json_string.replace(\"'\", \"\")\n",
    "\n",
    "try:\n",
    "    formatted_json = json.loads(json_string)\n",
    "    print(json.dumps(formatted_json, indent=3))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"Not valid JSON format:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando os resultados com a fatura (`invoice`) acima, notamos:\n",
    "\n",
    "* A saída do modelo é impecável — ele extrai todas as informações relevantes com precisão!\n",
    "\n",
    "* Isso ocorreu apesar da baixa qualidade da imagem e dos dados incorporados nas tabelas!\n",
    "\n",
    "* O `Qwen2-VL` menor teve um bom desempenho aqui, mas para imagens mais complexas ou texto manuscrito, você pode precisar de modelos maiores como `Qwen2-VL-7B`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">`Project 2:` Chat with Video</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `Qwen2-VL` também pode extrair informações e interagir com vídeos.\n",
    "\n",
    "Para este projeto, usaremos um pequeno vídeo do `YouTube` — uma cena famosa de Pulp Fiction com a dança de John Travolta e Uma Thurman:\n",
    "\n",
    "\n",
    "Library for downloading youtube videos:\n",
    "```\n",
    "%pip install pytubefix\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6MUy21DdnSc?controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Faça o download da seguinte forma:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "from pytubefix.cli import on_progress\n",
    "\n",
    "file_name = \"youtube_short.mp4\"\n",
    "\n",
    "url = \"https://youtube.com/shorts/6MUy21DdnSc?si=-GofdpXBc97pTQ-E\"\n",
    "\n",
    "yt = YouTube(url, on_progress_callback = on_progress)\n",
    "print(yt.title)\n",
    "\n",
    "ys = yt.streams.get_highest_resolution()\n",
    "ys.download(filename=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"youtube_short.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Usaremos `Qwen2-VL-2B` novamente, pois consome menos recursos.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62872e1367a744b28b7500ceaec91407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import json\n",
    "\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "     model_name,\n",
    "     torch_dtype=\"auto\",\n",
    "     ##attn_implementation=\"flash_attention_2\", #use flash-attention2 if your gpu card supports it (Free Colab's T4 does not support it)\n",
    "     device_map=\"cuda:0\", #\"auto\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos a função `chat_with_video`, que pega as `dimensões` do vídeo `redimensionado`, framess por segundo e a mensagem de texto que perguntaremos a `Qwen`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_video(file_name, query, video_width, video_height, fps=1.0):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": file_name,\n",
    "                    \"max_pixels\": video_width * video_height,\n",
    "                    \"fps\": 1.0,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": query},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    #inputs = inputs.to(\"cpu\")\n",
    "\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=150)\n",
    "\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Vamos perguntar ao modelo:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = chat_with_video(file_name, \"O que esse vídeo mostra?\", 360, 360)\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">E outra pergunta:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = chat_with_video(file_name, \"A mulher no vídeo está usando sapatos?\", 360, 420 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Surpreendentemente, a modelo respondeu ambas as perguntas com precisão!\n",
    "\n",
    "`Algumas observações adicionais:`\n",
    "\n",
    "* Aumentar a altura, a largura e a taxa de frames (`fps`) do vídeo geralmente melhora a precisão, mas requer mais `GPU VRAM`.\n",
    "\n",
    "* O `Qwen2-VL` pode processar vídeos com mais de `20 minutos`, mas ainda `não processa som`.\n",
    "\n",
    "* Em meus experimentos, o `Qwen2-VL-7B` oferece o melhor equilíbrio entre precisão e requisitos de recursos (`GPU VRAM`).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">`Project 3:` Multimodal RAG</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste projeto, combinaremos o `Qwen2-VL` com outro modelo, `ColPali`, para executar `RAG` com `PDFs`.\n",
    "\n",
    "`ColPali` é um modelo de recuperação (retrieval) de documentos que contém um modelo `PaliGemma-3B` (também `VLM`) e um `Gemma-2B`. A função do `ColPali` é executar a parte de recuperação de documentos e criar um repositório de documentos multivetorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4434fdcd-312b-47fc-9d38-d8a5a5181902_985x593.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Imagem superior:` Os sistemas de recuperação tradicionais usam ferramentas de `OCR` para extrair texto de imagens e, posteriormente, dividir o texto em `embeddings` de texto.\n",
    "\n",
    "* `Inferior: Imagem:` Em contraste, o `ColPali` executa `RAG` nos documentos visualmente (tratando a página do documento como uma imagem e dividindo-a em patches de imagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `pipeline` no nosso caso é o seguinte:\n",
    "\n",
    "* Converte cada página do `PDF` em uma imagem.\n",
    "\n",
    "* Insira as imagens no `ColPali` para armazenar uma representação `multivetorial` para cada página.\n",
    "\n",
    "* Envie uma consulta de texto ao `ColPali` para recuperar a(s) imagem(ns) relevante(s).\n",
    "\n",
    "* Envie a consulta de texto e as imagens relevantes para o `Qwen2-VL` para obter a resposta.\n",
    "\n",
    "\n",
    "\n",
    "Usaremos a biblioteca `Byaldi` para criar o `armazenamento de vetores de imagem`. O `Byaldi` carrega o `ColPali` (e modelos similares com a `API`). Também usaremos o `pdf2image` para converter o `PDF` em imagens.\n",
    "\n",
    "\n",
    "Vamos começar instalando as bibliotecas necessárias:\n",
    "\n",
    "\n",
    "```\n",
    "#pip install --upgrade byaldi\n",
    "\n",
    "pip install byaldi==0.0.5\n",
    "\n",
    "pip install -q git+https://github.com/huggingface/transformers.git qwen-vl-utils pdf2image\n",
    "\n",
    "\n",
    "\n",
    "## necessary tool for pdf2image\n",
    "\n",
    "!sudo apt-get install -y poppler-utils\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_All",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
